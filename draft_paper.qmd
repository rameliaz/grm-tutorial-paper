---
title: "Getting Started with the Graded Response Model (GRM): A gentle introduction and tutorial in R"
shorttitle: "A Graded Response Model Tutorial"
author:
  - name: Rizqy Amelia Zein
    corresponding: true
    orcid: 0000-0001-7840-0299
    email: amelia.zein@psikologi.unair.ac.id
    affiliations:
      - name: Universitas Airlangga
        department: Department of Psychology
        address: Jalan Airlangga 4-6
        city: Surabaya
        region: Jawa Timur, Indonesia
        postal-code: "60286"
      - name: Ludwig-Maximilians-Universität
        department: Department of Psychology
        address: Leopoldstraße 13
        city: Munich
        postal-code: "80802"
  - name: Hanif Akhtar
    orcid: 0000-0002-1388-7347
    affiliations:
      - name: ELTE Eötvös Loránd University
        department: Doctoral School of Psychology
        country: Hungary
      - name: Universitas Muhammadiyah Malang
        department: Faculty of Psychology
        city: Malang
        region: Jawa Timur, Indonesia
abstract: "This tutorial paper introduces the Graded Response Model (GRM), a tool for testing measurement precision under the Item Response Theory (IRT) paradigm. Addressing common problems of measurement imprecision and lack of construct validity, the tutorial guides researchers through a one-dimensional GRM analysis in the R environment, using psych, mirt, and ggmirt packages. GRM is specifically designed to examine the psychometric properties of psychological scales with polytomous items. The tutorial illustrates the procedure using data from the Open Psychometrics Database on the Right-Wing Authoritarianism (RWA) scale, outlining the theoretical underpinnings of GRM, and steps for data preparation, model fitting, interpretation of results, and dealing with common issues and anomalies that may typically arise in the process."
keywords:
  - graded response model
  - item response theory
  - mirt
  - ggmirt
  - psych
  - R
author-note:
  disclosures:
    conflict-of-interest: "We declare we have no competing interests."
    financial-support: "The first author receives a PhD scholarship from the German Academic Exchange Service (Deutscher Akademischer Austauschdienst, DAAD)."
lang: en
bibliography: references.bib
format:
  apaquarto-docx: default
editor: visual
execute: 
  warning: false
  error: false
  output: false
  echo: true
---

{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}

```{r}
#| include: false
#| label: "setup"

install.packages("flextable", dependencies=TRUE) 
library(flextable)
```

A foundation of scientific research is a measurement process, which starts from defining the construct being measured, determining measurement assumptions, and inspecting its measurement validity and precision. To illustrate this process, consider a researcher who designs a study aiming to identify the association between authoritarian disposition and prejudice toward a social group. As the first step, they must clearly define the variables they are interested in and determine a strategy to measure those variables accurately. Ensuring that the measurement strategy precisely measures what it is intended to measure often poses a challenge to applied psychology researchers because, most of the time, we have no direct access to observe most psychological constructs. Instead, we rely on statistical techniques, which assume that non-observable psychological constructs are "out there," or later we call this variable a *latent construct* or a *manifest variable*. More sharply, we create materials, such as scale items, stimuli, tasks, observation checklists, etc. that we *assume* to represent these underlying latent variables (or constructs) being measured. 

Relatedly, as most psychological constructs are not directly observable, the question on whether a scale actually measures what it is intended to measure (i.e., construct validity, see @cronbachConstructValidityPsychological1955 becomes critical. Construct validity, as argued by @cronbachConstructValidityPsychological1955, can be sustained by linking test scores to other constructs which researchers theoretically assume to be related to the construct of interest. Further, @cronbachConstructValidityPsychological1955 introduce the notion of a "nomological network," which refers to a theoretical framework connecting the measured construct with other (theoretically) related constructs and relevant observations. Therefore, the validity of the test depends on the behavior of the construct of interest: whether or not it behaves as specified in the theory within this network. To scrutinize test validity, researchers correlate the test score with some criterion (i.e., constructs or future and/or current behaviors theoretically related to the measured construct), and this is a widespread practice in psychology [@zumboSettingStageValidity2014]. 

However, validity theory has evolved over time and has been expanded since. Rather than a property of a specific test, scholars argue that the validity of a scale refers to questions about its consequential aspects [@kaneValidatingInterpretationsUses2013; @kaneValidityEvaluationClaims2016; @messickValidityPsychologicalAssessment1995]. In this sense, a test is deemed valid when researchers can demonstrate evidence for articulating a logical argument [@kaneValidatingInterpretationsUses2013; @kaneValidityEvaluationClaims2016] in defending test interpretations and its practical use [@messickValidityPsychologicalAssessment1995]. Consequently, scale validity is an ongoing process in which researchers should continually and incrementally gather (and report) the evidence as an important complement to ensure the credibility of their findings [@flakeConstructValidationSocial2017; @messickValidityPsychologicalAssessment1995; @zumboSettingStageValidity2014]. 

Yet in practice, researchers pay far less attention to the quality of their measures. According to a study examining articles published in a flagship journal in the field of social and personality psychology, only a little more than half (53%) of sample articles cited validity evidence from previous validation studies but 19% of those articles had adapted or modified the measures in various fashions so that the evidence for validity may not extend to the modified scales [@flakeConstructValidationSocial2017]. A larger study tapping into fifteen commonly used measures in social and personality psychology (e.g., Big Five Inventory, belief in just world scale, need for cognitive closure scale, etc.) even paints a grimmer picture [@husseyHiddenInvalidity152020]. While the majority of the tested scales (88%) purportedly had good validity, a more exhaustive examination (i.e., internal consistency, test-retest reliability, factor structure, and invariance) revealed that only 4% of these scales actually possess good measurement validity [@husseyHiddenInvalidity152020]. Furthermore, applied psychological researchers seem to be less aware of the assumptions underlying their chosen method for evaluating scale reliability. For example, while Cronbach's $\alpha$ is always almost reported [@flakeStrengtheningFoundationEducational2021] and continuously popular [@mcneishThanksCoefficientAlpha2018], it is unclear that researchers are aware of its underlying assumptions (that true scores and item variances should be equal and that item residuals should be uncorrelated, i.e., $\tau$ equivalence), and thus, limits its usefulness [@sijtsmaUseMisuseVery2008].

With this in mind, it is not surprising that researchers' lack of attention to their measurement quality raises questions about the replicability of their findings [@flakeMeasurementSchmeasurementQuestionable2020; @lilienfeldPsychologicalMeasurementReplication2020]. In this sense, a serious doubt about the validity of the measures used in a study can affect the validity of the findings, and the decisions that lead to this doubt are called *questionable measurement practice* (QMP, see @flakeMeasurementSchmeasurementQuestionable2020. The difficulty in reaching a consensus about the best way to measure a certain construct is also a major downstream consequence of the lack of validity reporting and may become a major obstacle to establishing cumulative psychological science [@elsonPsychologicalMeasuresAren2023]. 

Therefore, in our view, it is critical to equip applied psychological researchers with practical steps to systematically evaluate their measurement validity and precision. Moreover, since construct validation is an ongoing process [@messickValidityPsychologicalAssessment1995], applied researchers can contribute evidence for the validity of the scales they use in their research [@floraPurposePracticeExploratory2017] by implementing a GRM model to their data.  

In this article, we aim to gently introduce a graded response model (GRM), which is a family of item response models, specifically aimed to assess the measurement precision of a polytomous (Likert-style) scale. We start this paper by briefly explaining the basics of Item Response Theory (IRT), the theoretical foundation of GRM, as well as contrasting its differences with the widely known Classical True Score Theory (CTT). In this paper, we aim to keep our tutorial concise so we do not attempt to cover all IRT concepts in detail. Interested readers are referred to available didactic texts on IRT, such as @embretsonItemResponseTheory2000 for an introductory level and @bakerBasicsItemResponse2017 or @deayalaTheoryPracticeItem2022 for a more technical and comprehensive overview. 

Further, we briefly explain what GRM is and how a GRM model is specified. In this part, we highlight how model and item parameters are specified and interpreted as well as visualizations (i.e., plots) deriving from GRM analysis. In this part, we also further introduce some assumptions underlie a GRM and how to check these assumptions. Subsequently, we provide an illustrative example of testing measurement precision using GRM, and in this part, we show how to practically implement GRM analysis and interpret its results. We complement the illustrative example with code in the open-source programming language R [@rcoreteamLanguageEnvironmentStatistical2023] so that the readers can implement and reproduce the example presented in this paper. Readers with a little experience with R and RStudio can easily follow this tutorial but for those who are not yet familiar with R, we refer them to excellent, easy-to-follow materials designed by @navarroPsychologicalScience2018.

# A Brief Overview of Item Response Theory
To scrutinize their measurement, psychological researchers have relied upon two foundational principles underlying psychological testing (i.e., CTT and IRT) to decompose observed score into its *deterministic* and *random* (stochastic) elements [@zumboValidityFoundationalIssues2006]. On the one hand, CTT decomposes observed score into *true* score and its *error* components. In practice, researchers implement this principle by conducting Structural Equation Modeling (SEM), Exploratory Factor Analysis (EFA), or Confirmatory Factor Analysis (CFA). Specifying the measurement part of SEM, or employing EFA and CFA models, allows for the decomposition of the covariance matrix of a given data into latent factors, which represent true score or actual ability or trait level, and error terms (i.e., residuals or unique variances). Consequently, the relationship between observed score and true score is *linear*, as true score is essentially a linear transformation of observed score [@embretsonItemResponseTheory2000]. While on the other hand, IRT models this relationship in a *probabilistic* function rather than a linear one. More sharply, an IRT model assumes a probabilistic relationship between observed score and the latent trait being measured ($\Theta$), and notably, this probability accounts for the properties of the items, such as item discrimination, difficulty, and if necessary, guessing [@bakerBasicsItemResponse2017].

The most critical distinction between CTT and IRT is how they conceptualize measurement error. In CTT, standard deviation of the observed score (*s*) is used to calculate standard error of measurement. Since measurement error relies on *s*, CTT assumes that measurement error is sample-dependent and constant for all individuals in the sample regardless of their trait level. This shortcoming restricts the utility of CTT, especially since psychologists often need to interpret individual scores, not just evaluate the test as a whole. Additionally, since the standard error of measurement is also a function of reliability, which is typically denoted as a correlation between two parallel tests, reliability is presumed to be equal across different levels of a trait or ability. However, researchers frequently encounter situations where a test is excessively difficult for a group of low-performing participants, providing little information beyond indicating that their trait or ability level is significantly lower than what the test can measure.

IRT offers an elegant solution to this issue by enabling the calculation of standard errors for each individual (*SE*~$\theta$~), and test reliability can be inferred from the average of individual standard error across a sample of participants[ @embretsonItemResponseTheory2000; @langSciencePracticeItem2021]. This approach allows for the estimation of both overall reliability *and* reliability at varying levels of trait or ability. Returning to the previous example, when a test is overly challenging for low-performing individuals, IRT analysis can help identify the levels of trait or ability where the test is most reliable. A concrete example of this is a study scrutinizing the reliability of the Short Dark Tetrad (SD4) scale from an IRT perspective [@blotnerDarkTriadDead2022]. According to this study, the sadism subscale of the SD4 scale is the most reliable for measuring individuals with average to high levels ($\bar{x}$ < $\Theta$ < 2.5 *SD*) of sadism but suboptimal for measuring those with low levels of sadism [@blotnerDarkTriadDead2022]. Therefore, IRT models offer a more informative and nuanced perspective, proving useful for researchers who wish to closely examine the performance of their measures.

Further, IRT allows researchers to examine the performance of a specific item (hence "item" response theory) by specifying the relationship between item score and $\Theta$ given one-, two-, or three-parameter. One-parameter logistic (1PL) or Rasch model, the simplest variant of the IRT models, accounts only item difficulty (*b*) while presuming equal item discrimination (*a*) across items. Practically, this model slightly overlaps with CTT in the sense that they assume that all items carry equal informative value thus the estimated $\Theta$ of a 1PL/Rasch model is identical to the sum score of CTT [@langSciencePracticeItem2021; @stemlerRaschMeasurementItem2021]. This notion actually stems from a philosophical principle of *specific objectivity*, which requires that comparisons between measurements remain independent of both the item and individual characteristics [@raschSpecificObjectivityAttempt1977]. Put simply, the latent trait ($\Theta$) should remain unaffected by specific items used in the test. For example, the difference in depression levels between two individuals should be always the same regardless of the scale used to measure their depression levels. While this principle reasonably enforces objectivity in measurement practices, it demands a strong theory delineating the construct and strict requirements of data-model fit while both assumptions rarely hold in a real-world scenario, especially in psychological science.

Furthermore, the two-parameter logistic (2PL) model fills this gap by allowing the item ability to differentiate individuals with varying levels of $\Theta$ (i.e., item discrimination parameter - *a*) to differ. In some contexts, researchers may suspect that a part of the probabilistic relationship between observed score and $\Theta$ is explained by guessing thus three-parameter logistic (3PL) model incorporates guessing parameter (*c*). It is important to note that 1PL, 2PL, and 3PL models are only applicable to binary or dichotomous data (e.g., true/false response), and the focus of this article is nonetheless to show the utility of IRT for fitting the ordered (Likert-style) responses. The field of IRT has rapidly developed to include various models suited to specific contexts, such as handling ordinal responses [@murakiGeneralizedPartialCredit1992; @samejimaGradedResponseModel1997], categorical responses [@thissenNominalCategoriesItem2013], or assessing multidimensional traits simultaneously [@bockMarginalMaximumLikelihood1981; @chalmersMirtMultidimensionalItem2012]. We briefly summarize the features of the most frequently employed IRT models in {apatb-table1}.

# Graded Response Model
GRM is a family of IRT models specifically designed to analyze ordered polytomous (Likert-style) data [@samejimaGradedResponseModel1997; @samejimaGeneralGradedResponse2010; @samejimaGradedResponseModels2016]. The fundamental idea of GRM is to extend the logic of simpler dichotomous IRT models to polytomous scales by applying a probabilistic function to each response category. In dichotomous 2PL IRT models, the probability of answering a correct response is modeled as a function of ($\Theta$), item difficulty, and item discrimination. GRM extends this idea by modeling the probability of picking a certain response category or higher on an item (i.e., *step function*) given to item difficulty and discrimination. For each response category (e.g., answering "agree" on a five-point Likert scale), GRM calculates the cumulative probability of a participant answering "agree" or above, given to their $\Theta$. Therefore, the notion of item difficulty in dichotomous IRT is extended to the step function (i.e., item threshold).

To illustrate the step function, consider a scale measuring sadistic personality (e.g., "*watching a fist-fight excites me*," etc.) with response categories ranging from 1 (*strongly disagree*) to 4 (*strongly agree*). Imagine these categories are hierarchically ordered from the lowest category (*strongly disagree*) to the highest (*strongly agree*), like a staircase. GRM aims to estimate the level of sadistic personality ($\Theta$) required for "stepping" from one response category to another (e.g., *moderately agree* to *strongly agree*). Therefore, a GRM model of a five-point Likert scale calculates four item threshold parameters (*b*): the location of $\Theta$ level where individuals are equally likely to respond 1 or 2 (*b*~1~), 2 or 3 (*b*~2~), 3 or 4 (*b*~3~), and 4 or 5 (*b*~4~). Each threshold is exactly the point where a participant is equally likely to respond to either of the adjacent response categories. For example, if *b*~1~ = -1.23 for the item *watching a fist-fight excites me*, this means that participants with sadism level 1.23 below the mean are equally likely to either answer *strongly disagree* or *moderately disagree*.

Let's take a closer look at {apafg-fig1}, which shows an example of an item probability function (IPF) from an item with five response categories. {apafg-fig1} consists of five category probability curves, each of which represents the probability of endorsing a response category given $\Theta$. Item threshold is exactly a location of $\Theta$ where two adjacent category probability curves cross each other.

## Assumptions
Jelasin tiga asumsi penting: monotonicity, unidimensionality dan local independence. 

# Disclosure and Data Availability Statements
To maximize the reproducibility of our analysis, we wrote the article as a Quarto (.qmd) document, where we integrate the R codes used in the analysis as well as its outputs. We also include the complete R script for the example we used as a supplementary document. The Quarto file (and its corresponding .docx and .pdf output) and R script are publicly available on [a Github repository](https://github.com/rameliaz/grm-tutorial-paper). To demonstrate the use of GRM, we use a dataset obtained from [Open-Source Psychometrics Project](http://openpsychometrics.org/_rawdata/), which can be publicly accessed.

# An Illustrative Example of Graded Response Model: The Right Wing Authoritarianism (RWA) Scale

To demonstrate the procedure of running a graded response model, we

## A Brief Overview of the Altemeyer's RWA Scale

## Step 1: Preparation

```{r}
install.packages("tidyverse", "psych", "devtools", "mirt", "caret", dependencies=TRUE)
devtools::install_github("masurp/ggmirt") # remote installation through GitHub repository
```

Blabla

```{r}
library(ggmirt); library(tidyverse); library(psych); library(mirt); library(caret)
```

Blabla

```{r}
ds <- read.csv("data/data.csv")
```

Blabla

```{r}
str(ds)
```

Blabla

```{r}
rwa <- subset(ds, select = Q1:Q22)
str(rwa)
```

Blabla

## Step 2: Inspecting Key Descriptive Statistics

```{r}
psych::describe(rwa)
```

As we see in {apatb-table2}, Kok ada nilai 0-nya? padahal kan skor minimalnya 1. Coba kita hitung frekuensi nilai 0 di tiap item.

```{r}
zero <- colSums(rwa == 0) / nrow(rwa) * 100 # Computing the frequency of "0" in each column.
print(zero) # The proportion of "0" for each item.
```

Kita anggap nilai 0 adalah NA dan kita delete semua case dengan nilai NA

```{r}
rwa <- rwa %>%
  mutate_all(~na_if(., 0)) %>%  # Replacing 0 with NA in all columns.
  drop_na()  # Removing cases with any NA values.
```

Kalau lihat di {apatb-table2}, meannya kok beda2? oh iya karena ada unfavorable items. Ayo kita reverse score dulu ya.

```{r}
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
rwa <- rwa %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) + 1 to reverse code the unfavorable items.
```

## Step 3: Examining Dimensionality

```{r}
#| eval: false

irt.fa(rwa, nfactors = 1, fm = "minres")
```

lalalala

```{r}
#| eval: false

fa.parallel(rwa, nfactors = 1, fm="minres", fa="fa", cor = "poly")
```

lalalalal

```{r}
cor <- cor(rwa,method="pearson") # First, creating a (pearson) correlation matrix.
efa <- fa(rwa, nfactors=1, fm="minres") # Now, running exploratory factor analysis. 
print(efa) # Print the results.
```

Scree plot

Now lets do parallel analysis

```{r}
pa <- fa.parallel(rwa, fm="minres", fa="fa")
```

lalalala

```{r}
pa$fa.values
```

Eigenvalues `12.225/0.843`

## Step 4: Model Estimation, Parameters, and Fit Statistics

```{r}
model <- 'rwa = 1-22'
```

lalalala

```{r}
fit <- mirt(data=rwa, 1, model=model, itemtype="graded", SE=T, verbose=F)
```

llalalala

```{r}
coefs <- coef(fit, IRTpars=T, simplify=T) # Storing model parameters in a data frame.
print(coefs) # Yielding model parameters: item discrimination (a) and threshold (b).
```

lalalala

```{r}
summary(fit) 
```

bla bla lhalalala

```{r}
M2(fit, type="C2")
```

lhalalala

```{r}
#| include: false

m2_results <- M2(fit, type="C2")
m2_results[] <- lapply(m2_results, as.numeric)
m2_stat <- m2_results$M2
m2_stat = round(m2_stat, digits = 3)
df <- m2_results$df
p_value <- m2_results$p.value
rmsea <- m2_results$RMSEA
rmsea = round(rmsea, digits=3)
srmsr <- m2_results$SRMSR
srmsr = round(srmsr, digits=3)
tli <- m2_results$TLI
tli = round(tli, digits=3)
cfi <- m2_results$CFI
cfi = round(cfi, digits=3)
```

According to our analysis, the model does not fit the data well (*M*(`r df`) = `r m2_stat`, *p* = `r p_value`, *RMSEA* = `r rmsea`, )

```{r}
item.fit <- itemfit(fit)
```

## Step 5: Model Residuals

```{r}
ld <- residuals(fit, type = "LD") # Running local dependency statistics
up <- which(upper.tri(ld), arr.ind = T) # Extracting values only on the upper side of the diagonal.
lar <- up[ld[up] > 0.2 | ld[up] < -0.2, ] # Defining unusually large residuals (>0.2).
```

lhalalala

```{r}
#| eval: false

for (i in 1:nrow(lar)) {
  row <- lar[i, 1]
  col <- lar[i, 2]
  value <- ld[row, col]
  cat(sprintf("A large residual correlation is found between item %d and item %d: %f\n", row, col, value))
} # Now we detect the problematic pairs.
```

lhalalala

```{r}
q3 <- residuals(fit, type = "Q3") # Running Yen's Q3 statistics
findCorrelation(q3, cutoff = 0.2, verbose = T) # Detecting problematic correlation pairs.
```

## Step 6: IRT Plots

Now spit out the

```{r}
tracePlot(fit, facet=T, title = "Category Probability Functions of RWA Scale") + labs(color="Response Options")
```

lhalalala

```{r}
itemInfoPlot(fit, facet=T, title = "Item Information Curves of the RWA Scale")
```

lhalalala

```{r}
testInfoPlot(fit, title="Test Information Curve of the RWA Scale")
```

## Step 7: Computing Reliability

```{r}
m_rel <- marginal_rxx(fit)
```

lhalalala

```{r}
conRelPlot(fit, title="Reliability of the RWA Scale Given to the θ Level")
```

lalalala

```{r}
theta_se <- fscores(fit, full.scores.SE = T) # Extracting the estimated theta score of each participant.
e_rel <- empirical_rxx(theta_se) # Then use the estimated theta to calculate empirical reliability.
```

lhalalala

```{r}
omega(rwa)
```

# Conclusions

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

```{r}
#| label: apatb-table1
#| apa-cap: Comparison Between Common IRT Models
#| apa-note: NULL
#| echo: false
```

| Model                                   | Key Characteristics                                                                                                                                                                                                                                     | Data Type   | Response Options        |
|-----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|-------------------------|
| 1-PL Model (Rasch Model)                | 1. Estimates only item difficulties (*b*). 2. Assumes that all items have equal discrimination parameters (*a*). 3. Item and person parameters are independent.                                                                                          | Dichotomous | Correct/Incorrect (0/1) |
| 2-PL Model                              | 1. Estimates item difficulties (*b*) and item discrimination (*a*). 2. Less stringent than 1-PL model since it allows item discrimination parameters (*a*) to vary.                                                                                          | Dichotomous | Correct/Incorrect (0/1) |
| 3-PL Model                              | 1. Estimates item difficulties (*b*), discrimination (*a*), and pseudo-guessing parameter (*c*). 2. Appropriate for modeling a test data with multiple responses (e.g., multiple-choice tests), and thus, guessing might influence participants' responses. | Dichotomous | Correct/Incorrect (0/1) |
| Graded Response Model (GRM)             | 1. Appropriate for modeling ordinal data with more than two response categories (i.e., Likert-style). 2. Estimates a discrimination parameter (*a*) and multiple threshold parameters (*b*) per item.                                                       | Polytomous  | Ordered Categories      |
| Partial Credit Model (PCM)              | 1. An extension of the 1-PL (Rasch) model for polytomous items. 2. Estimates thresholds between adjacent categories but assumes equal discrimination across items.                                                                                      | Polytomous  | Ordered Categories      |
| Generalized Partial Credit Model (GPCM) | Extending PCM to allow differential discrimination parameters across items.                                                                                                                                                                             | Polytomous  | Ordered Categories      |
| Nominal Response Model (NRM)            | 1. Appropriate for modeling categorical responses with no order. 2. Estimates discrimination parameters (*a*) and multiple category-specific parameters (*b*).                                                                                              | Categorical | Unordered Categories    |

{{< pagebreak >}}

```{r}
#| label: apafg-fig1
#| apa-cap: An Item Probability Function from a GRM Model
#| apa-note: NULL
#| output: true
#| echo: false

knitr::include_graphics("CPF example.png")
```

{{< pagebreak >}}

```{r apatb-table2}
#| output: true
#| echo: false
#| apa-cap: Descriptive Statistics of RWA Scale
#| apa-note: SD = Standard Deviation
#| ft.align: left

# Transforming describe() output into a APA-formatted table
rwa <- subset(ds, select = Q1:Q22)
table1 <- psych::describe(rwa); table1 <- as.data.frame(table1)
table1 <- table1 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, mean, sd, min, max, range, n) %>% 
  rename(Mean=mean, SD=sd, Minimum=min, Maximum=max, Range=range) %>% 
  flextable() %>%
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table1 <- colformat_double(x=table1, j="Minimum", digits = 0)
table1 <- colformat_double(x=table1, j="Maximum", digits = 0)
table1 <- colformat_double(x=table1, j="Range", digits = 0)
table1 <- colformat_double(x=table1, j="n", digits = 0)
table1
```

{{< pagebreak >}}

```{r apatb-table3}
#| output: true
#| echo: false
#| apa-cap: Descriptive Statistics of RWA Scale
#| apa-note: Descriptive Statistics After Reversing Unfavorable Items and removing cases with NA. SD = Standard Deviation.
#| ft.align: left

rwa <- rwa %>%
  mutate_all(~na_if(., 0)) %>%  # Replacing 0 with NA in all columns.
  drop_na()  # Removing cases with any NA values.
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
rwa <- rwa %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) to reverse code the unfavorable items.
# Transforming describe() output into a APA-formatted table
table2 <- psych::describe(rwa); table2 <- as.data.frame(table2)
table2 <- table2 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, mean, sd, min, max, range, n) %>% 
  rename(Mean=mean, SD=sd, Minimum=min, Maximum=max, Range=range) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table2 <- colformat_double(x=table2, j="Minimum", digits = 0)
table2 <- colformat_double(x=table2, j="Maximum", digits = 0)
table2 <- colformat_double(x=table2, j="Range", digits = 0)
table2 <- colformat_double(x=table2, j="n", digits = 0)
table2
```

{{< pagebreak >}}

```{r fig2}
#| output: true
#| echo: false
#| label: apafg-fig2
#| apa-cap: Scree Plot
#| apa-note: NULL

plot(efa$values, type = "b", main = "Scree Plot", xlab = "Factor", ylab = "Eigenvalue") # Scree plot
abline(h = 1, col = "red", lty = 2) # Add new line to factor 1.

```

{{< pagebreak >}}

```{r fig3}
#| output: true
#| echo: false
#| label: apafg-fig3
#| apa-cap: Parallel Analysis
#| apa-note: NULL

plot(pa)
```

{{< pagebreak >}}

```{r}
#| include: false

sum <- summary(fit) 
```

```{r apatb-table4}
#| output: true
#| echo: false
#| apa-cap: Item Parameters
#| apa-note: λ = Standardized Factor Loadings, h2 = Commonality, α = Discrimination, β1.4 = Response specific difficulty parameters (item threshold).
#| ft.align: left
#| column: page

table3 <- as.data.frame(coefs$items)
load <- as.data.frame(sum$rotF)
h2 <- as.data.frame(sum$h2)
table3 <- cbind(table3, load, h2)
table3 <- table3 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, a, b1, b2, b3, b4, b5, b6, b7, b8, rwa, h2) %>% 
  rename(α=a, β1=b1, β2=b2, β3=b3, β4=b4, β5=b5, β6=b6, β7=b7, β8=b8, λ=rwa) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table3 <- colformat_double(x=table3, j="α", digits = 2)
table3 <- colformat_double(x=table3, j="β1", digits = 2)
table3 <- colformat_double(x=table3, j="β2", digits = 2)
table3 <- colformat_double(x=table3, j="β3", digits = 2)
table3 <- colformat_double(x=table3, j="β4", digits = 2)
table3 <- colformat_double(x=table3, j="β5", digits = 2)
table3 <- colformat_double(x=table3, j="β6", digits = 2)
table3 <- colformat_double(x=table3, j="β7", digits = 2)
table3 <- colformat_double(x=table3, j="β8", digits = 2)
table3

```

{{< pagebreak >}}

```{r apatb-table5}
#| output: true
#| echo: false
#| apa-cap: Item Fit Statistics
#| apa-note: Scaled χ2 Statistics. RMSEA = root mean square error of approximation, CFI = comparative fit index, TLI = Tucker-Lewis index, SRMR = Standardized Root Mean Square Residual.
#| ft.align: left

table4 <- as.data.frame(item.fit)
table4 <- table4 %>%
  select(item, S_X2, df.S_X2, RMSEA.S_X2, p.S_X2) %>% 
  rename(Item=item, χ2=S_X2, df=df.S_X2, RMSEA=RMSEA.S_X2, p=p.S_X2) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table4 <- colformat_double(x=table4, j="χ2", digits = 2)
table4 <- colformat_double(x=table4, j="df", digits = 2)
table4 <- colformat_double(x=table4, j="RMSEA", digits = 2)
table4 <- colformat_double(x=table4, j="p", digits = 2)
table4

```

{{< pagebreak >}}

```{r fig4}
#| output: true
#| echo: false
#| label: apafg-fig4
#| apa-cap: Item Probability Functions of RWA Scale
#| apa-note: NULL

tracePlot(fit, facet=T, title = "Item Probability Functions of RWA Scale") + labs(color="Response Options")
```

{{< pagebreak >}}

```{r fig5}
#| output: true
#| echo: false
#| label: apafg-fig5
#| apa-cap: Item Information Curves of the RWA Scale
#| apa-note: NULL

itemInfoPlot(fit, facet=T, title = "Item Information Curves of the RWA Scale")
```

{{< pagebreak >}}

```{r fig6}
#| output: true
#| echo: false
#| label: apafg-fig6
#| apa-cap: Test Information Curve of the RWA Scale
#| apa-note: NULL

testInfoPlot(fit, title="Test Information Curve of the RWA Scale")
```

{{< pagebreak >}}

```{r fig7}
#| output: true
#| echo: false
#| label: apafg-fig7
#| apa-cap: Reliability of the RWA Scale Given to the θ Level
#| apa-note: NULL

conRelPlot(fit, title="Reliability of the RWA Scale Given to the θ Level")
```
