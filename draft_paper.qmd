---
title: "Getting Started with the Graded Response Model (GRM): A gentle introduction and tutorial in R"
shorttitle: "A Graded Response Model Tutorial"
author:
  - name: Rizqy Amelia Zein
    corresponding: true
    orcid: 0000-0001-7840-0299
    email: amelia.zein@psikologi.unair.ac.id
    affiliations:
      - name: Universitas Airlangga
        department: Department of Psychology
        address: Jalan Airlangga 4-6
        city: Surabaya
        region: Jawa Timur, Indonesia
        postal-code: "60286"
      - name: Ludwig-Maximilians-Universität
        department: Department of Psychology
        address: Leopoldstraße 13
        city: Munich
        postal-code: "80802"
  - name: Hanif Akhtar
    orcid: 0000-0002-1388-7347
    affiliations:
      - name: ELTE Eötvös Loránd University
        department: Doctoral School of Psychology
        country: Hungary
      - name: Universitas Muhammadiyah Malang
        department: Faculty of Psychology
        city: Malang
        region: Jawa Timur, Indonesia
abstract: "This tutorial paper introduces the Graded Response Model (GRM), a tool for testing measurement precision under the Item Response Theory (IRT) paradigm. Addressing common problems of measurement imprecision and lack of construct validity, the tutorial guides researchers through a one-dimensional GRM analysis in the R environment, using psych, mirt, and ggmirt packages. GRM is specifically designed to examine the psychometric properties of psychological scales with polytomous items. The tutorial illustrates the procedure using data from the Open Psychometrics Database on the Right-Wing Authoritarianism (RWA) scale, outlining the theoretical underpinnings of GRM, and steps for data preparation, model fitting, interpretation of results, and dealing with common issues and anomalies that may typically arise in the process."
keywords:
  - graded response model
  - item response theory
  - mirt
  - ggmirt
  - psych
  - R
author-note:
  disclosures:
    conflict-of-interest: "We declare we have no competing interests."
    financial-support: "The first author receives a PhD scholarship from the German Academic Exchange Service (Deutscher Akademischer Austauschdienst, DAAD)."
lang: en
bibliography: references.bib
format:
  apaquarto-docx: default
editor: visual
execute: 
  warning: false
  error: false
  output: false
  echo: true
---

{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}

```{r}
#| include: false
#| label: "setup"

install.packages("flextable", dependencies=TRUE) 
library(flextable)
```

The foundation of scientific research is the measurement process, which involves defining the construct to be measured, determining measurement assumptions, and evaluating measurement validity and precision. For instance, a researcher studying the role of authoritarian personality in explaining prejudice must first clearly define these variables, and then, develop a measurement strategy to operationalize the variables. This is challenging in applied psychology, as most psychological constructs are not directly observable. To ensure a precise measurement, researchers rely on statistical techniques, assuming that these non-observable traits are "out there," or later we name this concept a *latent trait* or a *manifest variable*. Researchers then create tools, like scales, stimuli, tasks, observation checklists, etc., which are assumed to represent these these latent traits.

In psychological research, where variables are often not directly observable, construct validity, which refers to a question on whether a scale measures what it is intended to measure [@cronbachConstructValidityPsychological1955], becomes crucial. Construct validity, as argued by @cronbachConstructValidityPsychological1955, can be scrutinized by linking test scores to other related constructs and observations. Test validity relies on whether the construct behaves as theory predicts within a "nomological network" [@cronbachConstructValidityPsychological1955]. In practice, researchers establish validity by correlating the test score with external criterion i.e., other theoretically related traits or behavior [@zumboSettingStageValidity2014].

Over time, validity theory has evolved and has been expanded. Validity is now seen not as a property of a specific test, but rather lies in the consequential aspects of the test use [@kaneValidatingInterpretationsUses2013; @kaneValidityEvaluationClaims2016; @messickValidityPsychologicalAssessment1995]. In this sense, a test is deemed valid when researchers can demonstrate evidence for formulating a logical argument [@kaneValidatingInterpretationsUses2013; @kaneValidityEvaluationClaims2016], such as causal inference [@rutkowskiCausalInferencesLarge2016], used to defend test interpretations and its practical use [@messickValidityPsychologicalAssessment1995]. This makes construct validity an ongoing process where researchers are advised to continuously gather and report evidence for validity of the measures they use in their research [@flakeConstructValidationSocial2017; @messickValidityPsychologicalAssessment1995; @zumboSettingStageValidity2014].

Yet in practice, researchers tend to overlook to the quality of their measures. A study reviewing articles published in a social and personality psychology journal found that only a little more than half (53%) of sample articles cited validity evidence from previous validation studies. Nineteen percent of those articles, nonetheless, had adapted or modified the measures in many different ways so that the evidence for validity may not extend to these modified versions [@flakeConstructValidationSocial2017]. Additionally, a larger study examining fifteen commonly used measures in social and personality psychology (e.g., Big Five Inventory, belief in just world scale, need for cognitive closure scale, etc.) even paints a grimmer picture [@husseyHiddenInvalidity152020]. While the majority of the tested scales (88%) purportedly had good validity, a more exhaustive examination through internal consistency, test-retest reliability, factor structure, and invariance tests revealed that only 4% of these scales possess good measurement validity [@husseyHiddenInvalidity152020]. Furthermore, applied psychological researchers seem to overlook the assumptions underlying their chosen method for evaluating reliability. For example, while Cronbach's $\alpha$ is always almost reported [@flakeStrengtheningFoundationEducational2021] and exceptionally popular [@mcneishThanksCoefficientAlpha2018], it is unclear whether researchers are aware of its underlying assumptions (that true scores and item variances should be equal and that item residuals should be uncorrelated, i.e., $\tau$ equivalence), and thus, limits its usefulness [@sijtsmaUseMisuseVery2008].

With this in mind, this oversight in measurement quality can raise reasonable concerns about the replicability of research findings because measurement contributes to the (in)validity of the findings [@flakeMeasurementSchmeasurementQuestionable2020; @lilienfeldPsychologicalMeasurementReplication2020]. Measurement oversight can be identified in decisions that researchers make, which potentially compromise the validity of the measures used in their studies, a phenomenon known as "*questionable measurement practices*" [@flakeMeasurementSchmeasurementQuestionable2020]. Further, the lack of evidence for measurement validity contributes to the difficulty in reaching a consensus about the best way to measure a construct. In the long run, it becomes a major obstacle to establishing cumulative psychological science [@elsonPsychologicalMeasuresAren2023]. Given these circumstances, it is critical to provide applied psychological researchers with a set of tools and practical steps to assist them in evaluating measurement validity so that they can improve their measurement practices, and thus, the credibility of their findings.

In this article, we aim to gently introduce the graded response model (GRM), which is a family of item response models, specifically aimed to assess the measurement precision of a polytomous (Likert-style) scale. We start this paper by briefly explaining the basics of Item Response Theory (IRT), the theoretical foundation of GRM, as well as contrasting its differences with the widely known Classical True Score Theory (CTT). In this paper, we aim to keep our tutorial concise so we do not cover all IRT concepts in detail. Interested readers are referred to available didactic texts on IRT, such as @embretsonItemResponseTheory2000 for the introductory level, and @bakerBasicsItemResponse2017 and @deayalaTheoryPracticeItem2022 for a more technical and comprehensive overview.

Further, we briefly explain what GRM is and how a GRM model is specified. In this part, we highlight how model and item parameters are specified and interpreted as well as visualizations (i.e., plots) derived from a GRM analysis. We also introduce three key assumptions underlying a GRM and how to test these assumptions. Subsequently, we provide an illustrative example of testing measurement precision using GRM, and in this part, we show how to practically implement GRM analysis and interpret its results. We complement the illustrative example with code in the open-source programming language R [@rcoreteamLanguageEnvironmentStatistical2023] so that the readers can implement and reproduce the example presented in this paper. Readers with some experience with R and RStudio can easily follow this tutorial, but for those who are not yet familiar with R, we refer them to excellent, easy-to-follow materials developed by @navarroPsychologicalScience2018. At last, we present some concluding remarks to encourage researchers to implement a GRM analysis as a part of their routine.

# A Brief Overview of Item Response Theory

To scrutinize their measurement quality, psychological researchers have relied upon two foundational principles underlying psychological testing, i.e., CTT and IRT, to decompose observed score into its *deterministic* and *random* or stochastic elements [@zumboValidityFoundationalIssues2006]. On the one hand, CTT decomposes observed score into *true* score and its *error* components. In practice, researchers implement this principle by conducting Structural Equation Modeling (SEM), Exploratory Factor Analysis (EFA), or Confirmatory Factor Analysis (CFA). Specifying the measurement part of SEM, or employing EFA and CFA models, allows for the decomposition of the covariance matrix of a given data into latent factors, which represent true score or actual ability or trait level, and error terms (i.e., residuals or unique variances). Consequently, the relationship between observed score and true score is *linear*, as true score is essentially a linear transformation of observed score [@embretsonItemResponseTheory2000]. IRT, on the other hand, models this relationship in a *probabilistic* rather than a linear fashion. More sharply, an IRT model assumes a probabilistic relationship between the observed score and the latent trait being measured ($\theta$), and in particular, this probability takes into account the properties of the items, such as item discrimination, difficulty, and, if necessary, guessing [@bakerBasicsItemResponse2017].

The most critical distinction between CTT and IRT is how they conceptualize measurement error. In CTT, standard deviation of the observed score (*s*) is used to calculate standard error of measurement. Since measurement error relies on *s*, CTT assumes that measurement error is sample-dependent and constant for all individuals in the sample regardless of their trait level. This shortcoming restricts the utility of CTT, especially since psychologists often need to interpret individual scores, not just evaluate the test as a whole. Additionally, since standard error of measurement is also a function of reliability, which is typically denoted as a correlation between two parallel tests, reliability is presumed to be equal across different levels of a trait or ability. This is unrealistic since researchers frequently encounter situations where a test is excessively difficult for a group of low-performing participants, providing little information beyond indicating that their trait or ability level is significantly lower than what the test can measure.

IRT offers an elegant solution to this issue by enabling the calculation of standard errors for each individual (*SE*~$\theta$~), and test reliability can be inferred from the average of *SE*~$\theta$~ across a sample of participants [@embretsonItemResponseTheory2000; @langSciencePracticeItem2021]. This approach allows for the estimation of reliability at varying levels of trait or ability (i.e., test information function - TIF). By doing this, when a test is overly challenging for low-performing individuals or too easy for high-performing individuals, IRT analysis can help identify the levels of trait or ability where the test is most reliable. A concrete example of this is a study scrutinizing the reliability of the Short Dark Tetrad (SD4) scale from an IRT perspective [@blotnerDarkTriadDead2022]. According to this study, the sadism subscale of the SD4 scale is the most reliable for measuring individuals with average to high levels ($\bar{x}$ \< $\theta$ \< 2.5 *SD*) of sadism but suboptimal for measuring those with low levels of sadism [@blotnerDarkTriadDead2022]. Therefore, IRT models offer a more informative and nuanced perspective, proving useful for researchers who wish to closely examine the performance of their measures.

Further, IRT allows researchers to examine the performance of a specific item (hence "item" response theory) by specifying the relationship between item score and $\theta$ given one-, two-, or three-parameter. One-parameter logistic (1PL) or Rasch model, the simplest variant of the IRT models, accounts only item difficulty (*b*) while presuming equal item discrimination (*a*) across items. Practically, this model slightly overlaps with CTT in the sense that they assume that all items carry equal informative value thus the estimated $\theta$ of a 1PL/Rasch model is identical to the sum score of CTT [@langSciencePracticeItem2021; @stemlerRaschMeasurementItem2021]. This notion actually stems from a philosophical principle of *specific objectivity*, which requires that comparisons between measurements remain independent of both the item and individual characteristics [@raschSpecificObjectivityAttempt1977]. Put simply, Rasch model assumes that the latent trait ($\theta$) should remain unaffected by specific items used in the test. For example, the difference in depression levels between two individuals should be always the same regardless of the scale used to measure their depression levels. While this principle reasonably enforces objectivity in measurement practices, it demands a strong theory delineating the construct and strict requirements of data-model fit while both assumptions rarely hold in a real-world scenario, especially in psychological science.

Furthermore, the two-parameter logistic (2PL) model fills this gap by allowing item ability to differentiate individuals with varying levels of $\theta$ (i.e., item discrimination parameter - *a*) to differ. Additionally, in some contexts, researchers may suspect that a part of the probabilistic relationship between observed score and $\theta$ is explained by guessing thus three-parameter logistic (3PL) model incorporates guessing parameter (*c*). The field of IRT has rapidly developed to include various models suited to specific contexts, such as handling ordinal responses [@murakiGeneralizedPartialCredit1992; @samejimaGradedResponseModel1997], categorical responses [@thissenNominalCategoriesItem2013], or assessing multidimensional traits simultaneously [@bockMarginalMaximumLikelihood1981; @chalmersMirtMultidimensionalItem2012]. It is important to note that 1PL, 2PL, and 3PL models are only applicable to binary or dichotomous data (e.g., true/false response), and the focus of this article is nonetheless to show the utility of IRT for fitting the ordered (Likert-style) responses. We briefly summarize the features of the most frequently employed IRT models in {apatb-table1}.

# Graded Response Model

The GRM is a family of IRT models specifically designed to analyze ordered polytomous (Likert-style) data [@samejimaGradedResponseModel1997; @samejimaGeneralGradedResponse2010; @samejimaGradedResponseModels2016]. The fundamental idea of GRM is to extend the logic of simpler dichotomous IRT models to polytomous scales by applying a probabilistic function to each response category. In dichotomous 2PL IRT models, the probability of answering a correct response is modeled as a function of $\theta$, item difficulty, and item discrimination. GRM extends this idea by modeling the probability of picking a certain response category or higher on an item (i.e., *step function*) given to item difficulty and discrimination. For each response category (e.g., answering "agree" on a five-point Likert scale), GRM calculates the cumulative probability of a participant answering "agree" or above, given to their $\theta$. Therefore, the notion of item difficulty in dichotomous IRT models is extended to the step function (i.e., item threshold) in order to handle ordered polytomous data.

To illustrate the step function, consider a scale measuring sadistic personality (e.g., "*watching a fist-fight excites me*," etc.) with response categories ranging from 1 (*strongly disagree*) to 4 (*strongly agree*). Imagine these categories are hierarchically ordered from the lowest category (*strongly disagree*) to the highest (*strongly agree*), like a staircase. GRM aims to estimate the level of sadistic personality ($\theta$) required for "stepping" from one response category to another (e.g., *moderately agree* to *strongly agree*). Therefore, a GRM model of a five-point Likert scale calculates four item threshold parameters (*b*): the location of $\theta$ level where individuals are equally likely to respond 1 or 2 (*b*~1~), 2 or 3 (*b*~2~), 3 or 4 (*b*~3~), and 4 or 5 (*b*~4~). Each threshold is exactly the point where a participant is equally likely to respond to either of the adjacent response categories. For example, if *b*~1~ = -1.23 for the item "*watching a fist-fight excites me*", it implies that participants with sadism level 1.23 below the mean are equally likely to either answer *strongly disagree* or *moderately disagree*.

Let's take a closer look at {apafg-fig1}, which shows an example of an item probability function (IPF) from an item of a scale with five response categories. In general, IPF describes a relationship between $\theta$ (x-axis) and the probability of endorsing a response category (y-axis - *P*$\theta$). {apafg-fig1} consists of five category probability curves, each of which represents the probability of endorsing a response category given $\theta$. Item threshold is exactly the location of $\theta$ where two adjacent category probability curves cross each other.

## Assumptions

Before running an analysis, researchers need to be aware of assumption underlying a GRM model. The first key assumption is that $\theta$ is monotonically correlated with the probability of endorsing a response category. In other words, individuals with higher levels of $\theta$ have a higher probability of endorsing response category that represents greater intensity of the traits [@hambletonIRTModelsAnalysis2010]. For instance, people with higher tendency of sadistic personality have a greater chance of answering "*moderately agree*" or "*strongly agree*" on a sadism scale compared to those with lower levels of sadism.

The second key assumption pertains to unidimensionality of $\theta$. This implies that the item pool tested using a GRM analysis should represent only one latent trait ($\theta$). Although it is indeed possible to account for multiple $\theta$ in a IRT model (multidimensional IRT, see @bockFullInformationItemFactor1988, we limit the scope of our tutorial to the unidimensional GRM. An unidimensional IRT model can still be robustly applied to multidimensional data if multiple latent traits are moderately intercorrelated or when there is a strong general factor (*g*) underlying the data [@reiseEvaluatingImpactMultidimensionality2015]. To test whether an item pool represents only one $\theta$, researchers can apply a parallel analysis [@guoAssessingDimensionalityIRT2023] or a factor analysis [@hambletonAssessingDimensionalitySet1986], before specifying their GRM models.

Relatedly, a GRM model assumes that the item pool are locally independent, as the third assumption. Local independence suggests that participants' responses to one item do not influence their response to another. It is important to note that evidence for unidimensionality does not always warrant local independence. As an eyeball example - consider a trust in science scale containing these two items: "*I trust scientists working in the natural science*" and "*most biologists are trustworthy*." While both items might measure the same latent trait (i.e., trust in science), those who trust scientists working in the natural science are very likely to also trust biologists. To inspect whether items are locally independent, researchers can examine the relationships between item responses after accounting for $\theta$ using residual correlation analysis [@chenLocalDependenceIndexes1997].

# An Illustrative Example of Graded Response Model: The Right Wing Authoritarianism (RWA) Scale

## A Brief Overview of the Altemeyer's RWA Scale

To illustrate the application of GRM for analyzing measurement precision of an ordered polytomous scale, we provide an example of a GRM analysis of the RWA scale data [@altemeyerRightwingAuthoritarianism1981; @altemeyerAuthoritarians2006]. @altemeyerRightwingAuthoritarianism1981 defines RWA as a personality propensity to blindly abide established authorities (i.e., authoritarian submission), to act aggressively toward individuals who are perceived to be punished by these established authorities (i.e., authoritarian aggression), and to uphold traditional values promoted by the authorities (i.e., conventionalism). Individuals with higher levels of RWA tend to view the world as a dangerous place [@duckittDualProcessMotivational2017], and thus, to ensure stability and safety, they are motivated to preserve social order by deferring to those whom they perceive as legal, social, or moral authorities; for instance, the government, religious institutions, or political or religious or military leaders [@saundersRightWingAuthoritarianismScale2017]. The RWA scale developed by @altemeyerRightwingAuthoritarianism1981 remains as one of the most important measures in the field of social and political psychology [@saundersRightWingAuthoritarianismScale2017], and has helped researchers to conceptualize the role of RWA in shaping various social psychological outcomes, such as prejudice, political behavior, and various antisocial behaviors [@akramiRightWingAuthoritarianismSocial2006; @osbornePsychologicalCausesSocietal2023; @sibleyPersonalityPoliticalOrientation2012].

In this paper, we use the latest version of the RWA scale [@altemeyerAuthoritarians2006], which consists of 22 items that participants are asked to respond their agreement to those items on a nine-point scale, ranging from "*strongly disagree*" (-4) to "*strongly agree*" (+4). While the RWA scale consists of three sub-dimensions, i.e., submissiveness, aggression, and conventionalism, the RWA scale is (theoretically) unidimensional because [@altemeyerAuthoritarians2006] shows that the sub-dimensions are strongly intercorrelated. While some evidence shows otherwise [@duckittMultidimensionalityRightWingAuthoritarian2013], for the purpose of this article, we assume that the RWA scale is unidimensional.

# Disclosure and Data Availability Statements

To maximize the reproducibility of our illustrative example, we wrote the article as a Quarto (.qmd) document, where we integrate the R codes used in the analysis as well as its outputs. We also include the annotated R script file (.R) for the example we used as a supplementary document, which we highly recommend to use for a didactic purpose. The Quarto file (and its corresponding .docx and .pdf output) and R script file are publicly available on [a Github repository](https://github.com/rameliaz/grm-tutorial-paper). The dataset we used in this tutorial paper was obtained from [Open-Source Psychometrics Project](http://openpsychometrics.org/_rawdata/), which is publicly accessible.

## Step 1: Data Preparation

Before running the analysis, we need to install the necessary R packages, which are, *mirt* [@chalmersMirtMultidimensionalItem2023a; @chalmersMirtMultidimensionalItem2012], *psych* [@revellePsychProceduresPsychological2023a], *ggmirt* [@masurGgmirt2023], *tidyverse* [@wickhamWelcomeTidyverse2019], *caret* [@kuhnCaretClassificationRegression2024], and *devtools* [@wickhamDevtoolsToolsMake2022], with these following commands: 

```{r}
install.packages("tidyverse", # for data wrangling
                 "psych", # for descriptive statistics and unidimensionality test
                 "devtools", # for installing R package that is not available on CRAN
                 "mirt", # for conducting the main GRM analysis
                 "caret", # for helping us detecting large residuals correlation (local independence test)
                 dependencies=TRUE)
devtools::install_github("masurp/ggmirt") # remote ggmirt installation through GitHub repository
```

The above command also automatically requests to install dependency packages. Note that we install *ggmirt* directly from its GitHub repository because *ggmirt* is not yet available in the Comprehensive R Archive Network (CRAN). Next, we should activate the R packages by typing a command as follows:

```{r}
library(mirt); library(ggmirt); library(psych); library(tidyverse); library(caret)
```

Now after the packages are activated, we need to import the dataset to our R environment. The dataset (`data.csv`) and codebook (`codebook.txt`) are available in `data` folder in our repository. Readers can download them from our repository and then simply import the dataset with this following command:

```{r}
ds <- read.csv("data/data.csv")
```

However, since the dataset is openly accessible in the Open Psychometrics webpage, readers can also directly import the dataset from the Open Psychometrics webpage to their R environment. For interested readers, we show how to do this in detail in our annotated R script file.

Next, it is important to examine the structure of the data frame (`ds`) to get a glimpse of its structure. By typing `str()` command, readers can examine the variables' name, number of cases, types of variables (integer or character), and the values of each variable. Then readers can match the information provided in their R console to the codebook. 

```{r}
str(ds)
```

According the information provided in the codebook, the responses to the RWA scale are denoted in `Q1` to `Q22`, each represents score of each item of the RWA scale. Readers can also see the information of `Q1` to `Q22` variables in their R console: these variables are integer and have values ranging from 1 to 9. In the codebook, there is no information regarding what these values (i.e., 1-9) mean. However, we could reasonably assume that participants who chose "*strongly disagree*" (-4) were scored `1`, and those who opted for "*strongly agree*" (+4) were scored `9`. 

Please note that we are only interested in the responses of the RWA scale, so we should subset the dataset to contain only the variables we are interested in, and then, examine the structure again. To this end, readers can type these commands:

```{r}
rwa <- subset(ds, select = Q1:Q22)
str(rwa)
```

Here, we have a new data frame namely `rwa`, which contains only the responses to the RWA scale (`Q1:Q22`). In the data frame `rwa`, we have 22 variables with 9,881 participants. Before proceeding to the analysis stage, we recommend readers to inspect descriptive statistics, which we explain the procedure in the next sub-section.

## Step 2: Inspecting Key Descriptive Statistics

To efficiently examine descriptive statistics, readers can make use of `describe()` function from *psych* package [@revellePsychProceduresPsychological2023] by typing a simple command:

```{r}
describe(rwa)
```

This command summarizes key descriptive statistics of a data frame, and the summary is provided in {apatb-table2}. As we see in {apatb-table2}, all variables have minimum value of `0`, while the minimum score should be `1`. Again, there is no information provided in the codebook regarding what `0` actually represents. Therefore, before deciding what we should do next, it is better to find out how many `0` we have in the data frame. To this end, we can calculate the percentage of `0` in each item by running these commands:

```{r}
zero <- colSums(rwa == 0) / nrow(rwa) * 100 # Computing the percentage of "0" in each column.
print(zero) # Showing the percentage of "0" for each item.
```

After executing these commands, readers can view the percentage of `0` in each items in their R console, which overall are very small, ranging from 0.09% to 0.32% of the total cases. Since the percentages are very small, it is very likely that `0` is a code for a missing value. It is important to note that *mirt* package uses the Expected-Maximization (EM) algorithm to estimate model parameters, and the EM algorithm can handle missing values [@chalmersMirtMultidimensionalItem2012]. If we decide to eliminate cases with missing values, we still have a sizable number of cases, nonetheless. Therefore, to simplify our analysis, we only keep cases with complete responses by executing these commands:

```{r}
rwa <- rwa %>%
  mutate_all(~na_if(., 0)) %>%  # Recoding "0" with "NA" in all columns.
  drop_na()  # Removing cases with any "NA" values.
```

If readers prefer to keep cases with missing values, especially when the sample size is low, readers can exclude `drop_na()` command.

Interestingly, if we look {apatb-table2} again, mean score of the items seem to differ drastically. Some items have high mean scores, ranging from 6 to 7, but some of those are comparatively low. The RWA scale indeed has some unfavorable items but, again, there is no information in the codebook whether these unfavorable items have been reversely coded. However, if we look closely at ten items with high mean ($\bar{x}$ > 6), these are the items that are supposed to be unfavorable. Therefore, we can reasonably assume that unfavorable items have not been reversed in the original dataset, and we should do so by running these commands:

```{r}
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
rwa <- rwa %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) + 1 to reverse code the unfavorable items.
```

First, we define a vector namely `unfav` to identify which items should be reversed, and then we use `mutate()` function to reverse the score. This is simply done by subtracting the values of the unfavorable items from the maximum score plus one. We can examine descriptive statistics of our modified data frame by running `describe()` function again, and as readers can see in {apatb-table3}, now mean score across items do not drastically vary and have `1` as its minimum value. 

## Step 3: Examining Dimensionality

Next, we show the readers the ways of testing unidimensionality. To test the number of latent factors of an item pool, @deayalaAssessmentDimensionalityUse1991 suggests three alternatives, which are multidimensional scaling (MDS), EFA, and CFA. In this tutorial, we demonstrate the use of EFA, and *psych* package provides a very efficient way to do this is by calling this command:

```{r}
#| eval: false

irt.fa(rwa, nfactors = 1, fm = "minres")
```

`irt.fa` function is to run an EFA with a polychoric correlation matrix as an input, not a Pearson's correlation matrix, because our data is ordinal. The option `fm = "minres"` implies that we choose minimum residual as a factoring method due to its effectiveness in handling non-normally distributed data [@harmanFactorAnalysisMinimizing1966]. Alternatively, readers can also run a parallel analysis with a polychoric correlation matrix by calling this following command:

```{r}
#| eval: false

fa.parallel(rwa, nfactors = 1, fm = "minres", fa = "fa", cor = "poly")
```

A parallel analysis compares the eigenvalues from the data with those yielded from a randomly generated dataset with the same sample size in order to determine the number of factors one should retain in EFA [@guoAssessingDimensionalityIRT2023]. The codes indicate that we tell R to run a parallel analysis on the data frame `rwa`, with one factor. The option `fa="fa"` implies that we want to show the eigenvalues for a EFA, while `cor = "poly"` means that we use a polychoric correlation matrix as an input. 

The call to both commands above produces this following error: `Error in polychoric(x): You have more than8 categories for your items, polychoric is probably not needed`. Since we have more than eight response categories, we should instead run an EFA with a Pearson's correlation matrix as an input. Therefore, we first need to create a Pearson's correlation matrix from our data frame, and then, run a EFA by calling these following commands: 

```{r}
cor <- cor(rwa, method = "pearson") # First, creating a Pearson's correlation matrix.
efa <- fa(rwa, nfactors = 1, fm = "minres") # Now, running EFA.
print(efa) # Print the results.
```

After calling the commands, readers can find `Proportion Var` in the output, which reflects the amount of variance explained by a latent factor. The variance proportion of our EFA model is 0.56, which means that 56% of total variance is explained by a latent factor. Since the latent factor can account for more than 50% of variance, we can reasonably assume that the latent factor underlying our data is unidimensional. To better visualize the Eigenvalues of the EFA, we can draw a scree plot using these following commands:

```{r}
#| eval: false

plot(efa$values, type = "b", main = "Scree Plot", xlab = "Factor", ylab = "Eigenvalue") # Scree plot
abline(h = 1, col = "red", lty = 2) # Add new line to eigenvalue = 1.
```

The first line of code is a function to plot the Eigenvalues (`efa$values`, x-axis) given the number of latent factors (y-axis). The option `type = "b"` implies that we ask R to display the Eigenvalues exact points and a line connecting these points along the number of factors. The second line of code aims to display a horizontal red line (i.e., a reference line) to see which factors have eigenvalues greater than one, which is a common criterion for factor retention. {apafg-fig2} shows that there is a clear break or an "elbow" that displays the location where the Eigenvalues start to level off. This implies that factors beyond the first may not be meaningful and could be only noise. Therefore, the scree plot strengthens our assumption that the item pool is, indeed, unidimensional.

To scrutinize unidimensionality assumption, we can also run a parallel analysis by running this following command:

```{r}
pa <- fa.parallel(rwa, nfactors = 1, fm = "minres", fa = "fa")
```

Readers should find a plot similar to {apafg-fig3} appear right after executing this code as well as a message, which says: `Parallel analysis suggests that the number of factors =  6  and the number of components =  NA`. This means that the parallel analysis suggests that there are six factors underlying our item pool, not one. However, according to {apafg-fig3}, we can see clearly that the Eigenvalues from our actual data fall steeply after the first factor. To paint a clearer picture, we can examine the Eigenvalues of each latent factor estimated by the parallel analysis by running this following command:

```{r}
pa$fa.values
```

The output shows that the Eigenvalues for the first factor is much larger than the its adjacent factor (`12.225/0.843`), further supporting for the evidence for unidimensionality. 

## Step 4: Model Estimation, Parameters, and Fit Statistics

In this step, we start to specify a vector representing our model specification, such as:

```{r}
model <- 'rwa = 1-22'
```

This code implies that we want to estimate a model with $\theta$ namely `rwa`, and the items representing the $\theta$ are column 1 to 22 of the data frame that we are going to use in the analysis. Then, we can continue to model fitting by typing this following command:

```{r}
fit <- mirt(data = rwa, 1, model = model, itemtype = "graded", SE = TRUE, verbose = FALSE)
```

The code above implies that we are fitting an IRT model using data from a data frame namely `rwa` (`data = rwa`). The number `1` indicates that we presume that the model contains only one $\theta$, i.e., unidimensional. We also set the model specification to `model = model`, which we have previously determined. We set `itemtype` to `"graded"` because we want to run a GRM model. The `SE` option is set to `TRUE` because we ask R to also estimate the standard error. At last, we set the `verbose` option to `FALSE` to avoid R displaying unnecessary information during the model fitting process, so the `mirt()` function can run quietly.

Apart from this tutorial, readers might encounter four common error messages after running `mirt()` function: (a) `EM cycles terminated after 500 iterations`, (b) `Could not invert information matrix; model may not be empirically identified`, (c) `The following items have only one response category and cannot be estimated`, and (d) `Error in [<-(*tmp*, i, nms[[i]], value = items.old[[i]]) : subscript out of bounds`. Error messages (a) and (b) correspond to the convergence issue, while (c) and (d) are issues related to the data structure.

*mirt* uses the EM algorithm as a default option. In short, the EM algorithm estimates model parameters in two steps: the expectation (E) and the maximization (M) steps. In the E step, the algorithm estimates the expected value of the $\theta$, and in the M step, it improves model parameters to maximize the likelihood of the observed data given the expected value of $\theta$. The process continues until model parameters are converged in stable values that fit to the data. If the error message (a) appears, it means that the package cannot estimate a stable solution for model parameters after 500 iterations. As a solution, readers can simply increase the maximum number of iterations to, for instance, 1000 cycles by adding this option `technical = list(NCYCLES = 10000)` to the `mirt()` function. 

The solution to error message (b) is less straightforward than (a) because it indicates that the model cannot be identified at all, which can have several causes. It could be due to an error in specifying assumptions about the number of latent traits or item properties, or it could be due to insufficient sample size. Therefore, when error message (b) appears, it is important to take a step back and think twice about the model assumptions, the fit between the model and the underlying theory, and whether the sample size is large enough to estimate the model properly.

The error message (c) and (d) are easier to solve because both indicate the error are related to the data structure. The former means that certain items have no variance, consisting only one response category. The solution for this issue is to exclude these items from model estimations. The latter indicates a mismatch between the vector of model specification and the data frame, for instance, specifying a vector with ten variables, while in fact, the data frame has only nine columns. To solve this issue, readers are suggested to check again whether the specified vector representing the model specification matches the structure of the data frame.

Next, we can ask the package to calculate item discrimination and thresholds by calling these functions below:

```{r}
coefs <- coef(fit, IRTpars = TRUE, simplify = TRUE) # Saving model parameters (item discrimination (a) and threshold (b)) in a data frame.
print(coefs) # Calling the data frame.
summary(fit) # Displaying factor loadings and commonality.
```

The first line of code indicates that we are calling a `coef()` function to calculate item parameters from `fit` model, and then, store them in a new data frame called `coefs`. The `IRTpars` and `simplify` option is set to `TRUE` because we want to display the traditional IRT parameters (*a* and *b*) in a simple, readable format without showing the standard errors. 

The output from this procedure is presented in {apatb-table4}, which shows item discrimination (*a*) of each item and item thresholds (*b1-b8*) of each response category within an item. To interpret *a*, readers can use a rule of thumb suggested by @bakerBasicsItemResponse2017, which categorize item discrimination exceeding 0.00, 0.35, 0.65, 1.35, and 1.70, as very low, low, moderate, high, and very high. As we seen in {apatb-table4}, item discrimination parameters of the RWA scale are ranging from high to very high (1.57 (Q1) to 3.32 (Q7)), suggesting efficient differentiation among individuals at different RWA levels. All threshold parameters  show a consistent increase, implying that those with higher RWA levels tend towards higher response categories. However, *b1* and *b2* for all items are very close to zero, which indicates that individuals with RWA level around the mean are likely to opt for "*strongly disagree*" or stepping to its adjacent response category. This might suggest that the RWA scale is less sensitive to measure individuals with a low or very low RWA level. 

The third line of code is a function to calculate a factor loading ($\lambda$) and commonality (h^2^) of each item, which both are also provided in {apatb-table4}. Before estimating a graded response model, *mirt* ran an EFA, and now as we can see in the console, we are looking at the EFA results. The results are slightly different from our previous EFA analysis, because *mirt* runs EFA using a quasi polychoric correlation matrix, while the one we ran earlier to test unidimensionality used a Pearson's correlation matrix as an input. However, most importantly, we see that all items are significantly loaded to one factor, and the factor now substantially accounts for 65.1% of the variance in the data, which strengthens our assumption that the RWA scale is unidimensional.

Next, we can estimate the model goodness-of-fit (GOF) statistics (*M*~2~) by calling this command below:

```{r}
M2(fit, type = "C2")
```
```{r}
#| include: false

m2_results <- M2(fit, type="C2")
m2_results[] <- lapply(m2_results, as.numeric)
m2_stat <- m2_results$M2
m2_stat = round(m2_stat, digits = 3)
df <- m2_results$df
p_value <- m2_results$p.value
rmsea <- m2_results$RMSEA
rmsea = round(rmsea, digits=3)
srmsr <- m2_results$SRMSR
srmsr = round(srmsr, digits=3)
tli <- m2_results$TLI
tli = round(tli, digits=3)
cfi <- m2_results$CFI
cfi = round(cfi, digits=3)
```

We ask the package to calculate the scaled *M*~2~ statistics [@maydeu-olivaresLimitedInformationGoodnessoffit2006] and other GOF indices, such as Root Mean Square Error of Approximation (RMSEA), Comparative Fit Index (CFI), Tucker-Lewis Index (TLI), and Standardized Root-Mean-Square Residual (SRMSR) by running this code above. Here, we set the `type` option to `"C2"` because it is suitable for computing *M*~2~ statistics in polytomous models. The interpretation of GOF statistics are similar to a general guide of interpreting GOF in CFA or SEM models [@schermelleh-engelEvaluatingFitStructural2003].

According to our analysis, the model overall does not fit the data well (*M*~2~(`r df`) = `r m2_stat`, *p* = `r p_value`, *RMSEA* = `r rmsea`, *SRMSR* = `r srmsr`, TLI = `r tli`, CFI = `r cfi`), and this may be caused by local dependency between the items. We will explore this issue further in the next part.

Next, we calculate item-level fit statistics, i.e., the scaled $\chi$^2^ statistics, by running this following function:

```{r}
item.fit <- itemfit(fit) # Estimating item fit statistics and saving them in a data frame.
print(item.fit) # Calling the data frame.
```

The output of this function is provided in {apatb-table5}, which shows fit statistics for an individual item. All items have very good (low) *RMSEA* values but normally, we want the *p* values of the scaled $\chi$^2^ to be nonsignificant, implying there is no discrepancy between the observed response pattern with what the model predicts. In {apatb-table5}, only three items here are nonsignificant (Q2, Q4, and Q12), which strengthens our suspicion that the RWA items might not be locally independent. 

## Step 5: Model Residuals

To ensure that the items are locally independent, *mirt* offers several alternatives to examine the behavior of model residuals. First, we demonstrate how to run Local Dependency (LD) $\chi$^2^ statistics [@chenLocalDependenceIndexes1997], which looks at the covariance between item pairs after accounting for $\theta$. To that end, readers can run two lines of codes as follows:

```{r}
ld <- residuals(fit, type = "LD") # Running local dependency (LD) statistics.
up <- which(upper.tri(ld), arr.ind = TRUE) # Creating a new matrix containing values only on the upper side of the diagonal (correlations).
```

The first line of code aims to extract residuals of `fit` model and store them as a matrix, namely `ld`. The `type` option is set to `"LD"` because we ask the package to calculate LD statistics of the `fit` model. After running the first line, readers may view a matrix containing residual covariance (in the lower diagonal) and correlations (in the upper diagonal). Correlations are more interpretable, so we focus on the upper side of the diagonal. Therefore, the second line of code is to create a new matrix containing only correlation values in the upper diagonal.

We are only interested in large correlations between a pair of items because they indicate that responses to one item depend on responses to another item beyond what is explained by $\theta$. It is important to note that there is no consensus on a rule of thumb for the cutoff value of residual correlations because LD statistics are contingent to the parameters of a specific dataset [@christensenCriticalValuesYen2017]. For the purpose of this tutorial, we set |0.2| as an paired correlation value that we want to look closely. To do so, we create a vector to define the value of residual correlations that we are interested in:

```{r}
#| eval: false
lar <- up[ld[up] > 0.2 | ld[up] < -0.2, ] # A vector defining unusually large residuals ( > |0.2|).
```

We can then use the new vector `lar` to ask R to tell us the item pairs with residuals correlations above |0.2| by running this function:

```{r}
#| eval: false
for (i in 1:nrow(lar)) {
  row <- lar[i, 1]
  col <- lar[i, 2]
  value <- ld[row, col]
  cat(sprintf("A large residual correlation is found between item %d and item %d: %f\n", row, col, value))
} # Now we detect the problematic pairs.
```

After running the function above, readers may encounter an error message: `Error in lar[i, 1] : subscript out of bounds`. This message indicates that R does not detect any residual correlation above |0.2|. To paint a clearer picture, we may need to use another way to examine model residuals. In the next part, we demonstrate the use of Yen's *Q*~3~ statistics [@yenEffectsLocalItem1984], which is considered more powerful for detecting underlying local dependency [@chenLocalDependenceIndexes1997]. To run Yen's *Q*~3~, readers can type and run this command:

```{r}
q3 <- residuals(fit, type = "Q3") # Running Yen's Q3 statistics.
```

This code asks R to run `residuals()` function using `"Q3"` as the `type`, and then, keep the value as a new matrix, namely `q3`. Again, we are only interested in item pairwise correlations above |0.2|, and to help us flag problematic pairs, we can make use of `findCorrelation()` function from *caret* package [@kuhnCaretClassificationRegression2024] as follows:

```{r}
findCorrelation(q3, cutoff = 0.2, verbose = T) # Flagging problematic item pairwise correlations.
```

After running the code, readers may see in their R console that residual correlations between item Q3, Q4, Q5, Q7, Q11, Q13, Q14, Q18, Q19, and Q21 are above |0.2|. Interestingly, all of those items, except for Q4, have nonsignificant *S*-$\chi$^2^ statistics (see {apatb-table5}) as well, which tells us that we have to look at these items closely and then decide whether these items have shared characteristics beyond what is explained by the model.

## Step 6: IRT Plots

*mirt* offers features to visualize model parameters but the options for customizing these plots are rather limited. *ggmirt* package [@masurGgmirt2023] fills this gap by combining *mirt* and *ggplot2* functions. In this tutorial, we demonstrate the use of *ggmirt* to visualize Item Probability Function (IPF), Item Information Function (IIF), Test Information Function (TIF), and conditional probability plot.

To visualize IPF, readers can type and run this function:

```{r}
tracePlot(fit, title = "Item Probability Functions of RWA Scale") + labs(color="Response Categories")
```

The `labs(color)` function is to assign different colors to each curve representing a response category. The output of this function is {apafg-fig4}. In {apafg-fig4}, we see that as $\theta$ increases, respondents are more likely to choose higher response categories. The gradual transition from one response option to the next across the spectrum of $\theta$ values indicates that the items are capturing incremental increases in the RWA level. However, all IPFs of RWA items seem to be significantly overlapping and tend to peak on a $\theta$ value close to or higher than the mean. This implies that RWA items are more sensitive to differentiate participants with high levels of RWA.

To evaluate the performance of each item in measuring RWA trait, we can visualize the amount of information explained by each item with this simple line of code:

```{r}
itemInfoPlot(fit, facet = TRUE, title = "Item Information Functions of the RWA Scale")
```

The `facet` option is set to `TRUE` so that R displays IIF for each item. The output for this function is {apafg-fig5}, where the x-axis represents the range of $\theta$, and the y-axis indicates the amount of information provided by each item. Peaks that are higher and narrower indicate items that are very informative for specific levels of $\theta$. In our case, most items provide the highest amount of information near the center to the right side of the $\theta$ distribution, indicating they are most useful for individuals with an average to high level of RWA.

It is also possible to evaluate the overall performance of the RWA scale by plotting a TIF, which can be done by running this following code: 

```{r}
testInfoPlot(fit, title = "Test Information Function of the RWA Scale")
```

The output of this code is {apafg-fig6}, and according to this, the RWA scale as a whole is informative for measuring individuals with a wide range of RWA levels, i.e., between -2*SD* to +4*SD*. 

## Step 7: Computing Reliability

```{r}
m_rel <- marginal_rxx(fit)
```

lhalalala

```{r}
conRelPlot(fit, title = "Reliability of the RWA Scale Given to the θ Level")
```

{apafg-fig7}

```{r}
theta_se <- fscores(fit, full.scores.SE = T) # Extracting the estimated theta score of each participant.
e_rel <- empirical_rxx(theta_se) # Then use the estimated theta to calculate empirical reliability.
```

lhalalala

```{r}
omega(rwa)
```

# Conclusions

Since construct validation is an ongoing process [@messickValidityPsychologicalAssessment1995], applied researchers can contribute to the cumulative science by reporting evidence for measurement validity [@floraPurposePracticeExploratory2017], and this can be done by implementing and reporting a GRM analysis.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

```{r}
#| label: apatb-table1
#| apa-cap: Comparison Between Common IRT Models
#| apa-note: NULL
#| echo: false
```

| Model                                   | Key Characteristics                                                                                                                                                                                                                                          | Data Type   | Response Options        |
|----------|--------------------------------------------|----------|----------|
| 1-PL Model (Rasch Model)                | 1\. Estimates only item difficulties (*b*). 2. Assumes that all items have equal discrimination parameters (*a*). 3. Item and person parameters are independent.                                                                                             | Dichotomous | Correct/Incorrect (0/1) |
| 2-PL Model                              | 1\. Estimates item difficulties (*b*) and item discrimination (*a*). 2. Less stringent than 1-PL model since it allows item discrimination parameters (*a*) to vary.                                                                                         | Dichotomous | Correct/Incorrect (0/1) |
| 3-PL Model                              | 1\. Estimates item difficulties (*b*), discrimination (*a*), and pseudo-guessing parameter (*c*). 2. Appropriate for modeling a test data with multiple responses (e.g., multiple-choice tests), and thus, guessing might influence participants' responses. | Dichotomous | Correct/Incorrect (0/1) |
| Graded Response Model (GRM)             | 1\. Appropriate for modeling ordinal data with more than two response categories (i.e., Likert-style). 2. Estimates a discrimination parameter (*a*) and multiple threshold parameters (*b*) per item.                                                       | Polytomous  | Ordered Categories      |
| Partial Credit Model (PCM)              | 1\. An extension of the 1-PL (Rasch) model for polytomous items. 2. Estimates thresholds between adjacent categories but assumes equal discrimination across items.                                                                                          | Polytomous  | Ordered Categories      |
| Generalized Partial Credit Model (GPCM) | Extending PCM to allow differential discrimination parameters across items.                                                                                                                                                                                  | Polytomous  | Ordered Categories      |
| Nominal Response Model (NRM)            | 1\. Appropriate for modeling categorical responses with no order. 2. Estimates discrimination parameters (*a*) and multiple category-specific parameters (*b*).                                                                                              | Categorical | Unordered Categories    |

{{< pagebreak >}}

```{r}
#| label: apafg-fig1
#| apa-cap: An Item Probability Function from a GRM Model
#| apa-note: NULL
#| output: true
#| echo: false

knitr::include_graphics("CPF example.png")
```

{{< pagebreak >}}

```{r apatb-table2}
#| output: true
#| echo: false
#| apa-cap: Descriptive Statistics of RWA Scale
#| apa-note: SD = Standard Deviation
#| ft.align: left

# Transforming describe() output into a APA-formatted table
rwa <- subset(ds, select = Q1:Q22)
table1 <- psych::describe(rwa); table1 <- as.data.frame(table1)
table1 <- table1 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, mean, sd, min, max, range, n) %>% 
  rename(Mean=mean, SD=sd, Minimum=min, Maximum=max, Range=range) %>% 
  flextable() %>%
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table1 <- colformat_double(x=table1, j="Minimum", digits = 0)
table1 <- colformat_double(x=table1, j="Maximum", digits = 0)
table1 <- colformat_double(x=table1, j="Range", digits = 0)
table1 <- colformat_double(x=table1, j="n", digits = 0)
table1
```

{{< pagebreak >}}

```{r apatb-table3}
#| output: true
#| echo: false
#| apa-cap: Descriptive Statistics of RWA Scale
#| apa-note: Descriptive Statistics After Reversing Unfavorable Items and removing cases with NA. SD = Standard Deviation.
#| ft.align: left

rwa <- rwa %>%
  mutate_all(~na_if(., 0)) %>%  # Replacing 0 with NA in all columns.
  drop_na()  # Removing cases with any NA values.
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
rwa <- rwa %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) to reverse code the unfavorable items.
# Transforming describe() output into a APA-formatted table
table2 <- psych::describe(rwa); table2 <- as.data.frame(table2)
table2 <- table2 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, mean, sd, min, max, range, n) %>% 
  rename(Mean=mean, SD=sd, Minimum=min, Maximum=max, Range=range) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table2 <- colformat_double(x=table2, j="Minimum", digits = 0)
table2 <- colformat_double(x=table2, j="Maximum", digits = 0)
table2 <- colformat_double(x=table2, j="Range", digits = 0)
table2 <- colformat_double(x=table2, j="n", digits = 0)
table2
```

{{< pagebreak >}}

```{r fig2}
#| output: true
#| echo: false
#| label: apafg-fig2
#| apa-cap: Scree Plot
#| apa-note: NULL

plot(efa$values, type = "b", main = "Scree Plot", xlab = "Factor", ylab = "Eigenvalue") # Scree plot
abline(h = 1, col = "red", lty = 2) # Add new line to factor 1.

```

{{< pagebreak >}}

```{r fig3}
#| output: true
#| echo: false
#| label: apafg-fig3
#| apa-cap: Parallel Analysis
#| apa-note: NULL

plot(pa)
```

{{< pagebreak >}}

```{r}
#| include: false

sum <- summary(fit) 
```

```{r apatb-table4}
#| output: true
#| echo: false
#| apa-cap: Item Parameters
#| apa-note: a = Discrimination, b1-b8 = Response specific threshold parameters, λ = Standardized Factor Loadings, h2 = Commonality.
#| ft.align: left
#| column: page

table3 <- as.data.frame(coefs$items)
load <- as.data.frame(sum$rotF)
h2 <- as.data.frame(sum$h2)
table3 <- cbind(table3, load, h2)
table3 <- table3 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, a, b1, b2, b3, b4, b5, b6, b7, b8, rwa, h2) %>% 
  rename(λ=rwa) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table3 <- colformat_double(x=table3, j="a", digits = 2)
table3 <- colformat_double(x=table3, j="b1", digits = 2)
table3 <- colformat_double(x=table3, j="b2", digits = 2)
table3 <- colformat_double(x=table3, j="b3", digits = 2)
table3 <- colformat_double(x=table3, j="b4", digits = 2)
table3 <- colformat_double(x=table3, j="b5", digits = 2)
table3 <- colformat_double(x=table3, j="b6", digits = 2)
table3 <- colformat_double(x=table3, j="b7", digits = 2)
table3 <- colformat_double(x=table3, j="b8", digits = 2)
table3

```

{{< pagebreak >}}

```{r apatb-table5}
#| output: true
#| echo: false
#| apa-cap: Item Fit Statistics
#| apa-note: Scaled χ2 Statistics. RMSEA = Root Mean Square Error of Approximation, CFI = Comparative Fit Index, TLI = Tucker-Lewis Index, SRMR = Standardized Root Mean Square Residual.
#| ft.align: left

table4 <- as.data.frame(item.fit)
table4 <- table4 %>%
  select(item, S_X2, df.S_X2, RMSEA.S_X2, p.S_X2) %>% 
  rename(Item=item, χ2=S_X2, df=df.S_X2, RMSEA=RMSEA.S_X2, p=p.S_X2) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table4 <- colformat_double(x=table4, j="χ2", digits = 2)
table4 <- colformat_double(x=table4, j="df", digits = 2)
table4 <- colformat_double(x=table4, j="RMSEA", digits = 2)
table4 <- colformat_double(x=table4, j="p", digits = 2)
table4

```

{{< pagebreak >}}

```{r fig4}
#| output: true
#| echo: false
#| label: apafg-fig4
#| apa-cap: Item Probability Functions of RWA Scale
#| apa-note: NULL

tracePlot(fit, facet=T, title = "Item Probability Functions of RWA Scale") + labs(color="Response Options")
```

{{< pagebreak >}}

```{r fig5}
#| output: true
#| echo: false
#| label: apafg-fig5
#| apa-cap: Item Information Curves of the RWA Scale
#| apa-note: NULL

itemInfoPlot(fit, facet=T, title = "Item Information Curves of the RWA Scale")
```

{{< pagebreak >}}

```{r fig6}
#| output: true
#| echo: false
#| label: apafg-fig6
#| apa-cap: Test Information Curve of the RWA Scale
#| apa-note: NULL

testInfoPlot(fit, title="Test Information Curve of the RWA Scale")
```

{{< pagebreak >}}

```{r fig7}
#| output: true
#| echo: false
#| label: apafg-fig7
#| apa-cap: Reliability of the RWA Scale Given to the θ Level
#| apa-note: NULL

conRelPlot(fit, title="Reliability of the RWA Scale Given to the θ Level")
```
