---
title: "Getting Started with the Graded Response Model (GRM): A gentle introduction and tutorial in R"
shorttitle: "A Graded Response Model Tutorial"
author:
  - name: Rizqy Amelia Zein
    corresponding: true
    orcid: 0000-0001-7840-0299
    email: amelia.zein@psikologi.unair.ac.id
    affiliations:
      - name: Universitas Airlangga
        department: Department of Psychology
        address: Jalan Airlangga 4-6
        city: Surabaya
        region: Jawa Timur, Indonesia
        postal-code: "60286"
      - name: Ludwig-Maximilians-Universität
        department: Department of Psychology
        address: Leopoldstraße 13
        city: Munich
        postal-code: "80802"
  - name: Hanif Akhtar
    orcid: 0000-0002-1388-7347
    affiliations:
      - name: ELTE Eötvös Loránd University
        department: Doctoral School of Psychology
        country: Hungary
      - name: Universitas Muhammadiyah Malang
        department: Faculty of Psychology
        city: Malang
        region: Jawa Timur, Indonesia
abstract: "This tutorial paper introduces the Graded Response Model (GRM), a tool for testing measurement precision under the Item Response Theory (IRT) paradigm. Addressing common problems of measurement imprecision and lack of construct validity, the tutorial guides researchers through a one-dimensional GRM analysis in the R environment, using psych, mirt, and ggmirt packages. GRM is specifically designed to examine the psychometric properties of psychological scales with polytomous items. The tutorial illustrates the procedure using data from the Open Psychometrics Database on the Right-Wing Authoritarianism (RWA) scale, outlining the theoretical underpinnings of GRM, and steps for data preparation, model fitting, interpretation of results, and dealing with common issues and anomalies that may typically arise in the process."
keywords:
  - graded response model
  - item response theory
  - mirt
  - ggmirt
  - psych
  - R
author-note:
  disclosures:
    conflict-of-interest: "We declare we have no competing interests."
    financial-support: "The first author receives a PhD scholarship from the German Academic Exchange Service (Deutscher Akademischer Austauschdienst, DAAD)."
lang: en
bibliography: references.bib
format:
  apaquarto-docx: default
editor: visual
execute: 
  warning: false
  error: false
  output: false
  echo: true
---

{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}

```{r}
#| include: false
#| label: "setup"

install.packages("flextable", dependencies=TRUE) 
library(flextable)
```

The foundation of scientific research is the measurement process, which involves defining the construct to be measured, determining measurement assumptions, and evaluating measurement validity and precision. For instance, a researcher studying the role of authoritarian personality in explaining prejudice must first clearly define these variables, and then, develop a measurement strategy to operationalize the variables. This is challenging in applied psychology, as most psychological constructs are not directly observable. To ensure a precise measurement, researchers rely on statistical techniques, assuming that these non-observable traits are "out there," or later we name this concept a *latent trait* or a *manifest variable*. Researchers then create tools, like scales, stimuli, tasks, observation checklists, etc., which are assumed to represent these these latent traits.

In psychological research, where variables are often not directly observable, construct validity, which refers to a question on whether a scale measures what it is intended to measure [@cronbachConstructValidityPsychological1955], becomes crucial. Construct validity, as argued by @cronbachConstructValidityPsychological1955, can be scrutinized by linking test scores to other related constructs and observations. Test validity relies on whether the construct behaves as theory predicts within a "nomological network" [@cronbachConstructValidityPsychological1955]. In practice, researchers establish validity by correlating the test score with external criterion i.e., other theoretically related traits or behavior [@zumboSettingStageValidity2014].

Over time, validity theory has evolved and has been expanded. Validity is now seen not as a property of a specific test, but rather lies in the consequential aspects of the test use [@kaneValidatingInterpretationsUses2013; @kaneValidityEvaluationClaims2016; @messickValidityPsychologicalAssessment1995]. In this sense, a test is deemed valid when researchers can demonstrate evidence for formulating a logical argument [@kaneValidatingInterpretationsUses2013; @kaneValidityEvaluationClaims2016], such as causal inference [@rutkowskiCausalInferencesLarge2016], used to defend test interpretations and its practical use [@messickValidityPsychologicalAssessment1995]. This makes construct validity an ongoing process where researchers are advised to continuously gather and report evidence for validity of the measures they use in their research [@flakeConstructValidationSocial2017; @messickValidityPsychologicalAssessment1995; @zumboSettingStageValidity2014].

Yet in practice, researchers tend to overlook to the quality of their measures. A study reviewing articles published in a social and personality psychology journal found that only a little more than half (53%) of sample articles cited validity evidence from previous validation studies. Nineteen percent of those articles, nonetheless, had adapted or modified the measures in many different ways so that the evidence for validity may not extend to these modified versions [@flakeConstructValidationSocial2017]. Additionally, a larger study examining fifteen commonly used measures in social and personality psychology (e.g., Big Five Inventory, belief in just world scale, need for cognitive closure scale, etc.) even paints a grimmer picture [@husseyHiddenInvalidity152020]. While the majority of the tested scales (88%) purportedly had good validity, a more exhaustive examination through internal consistency, test-retest reliability, factor structure, and invariance tests revealed that only 4% of these scales possess good measurement validity [@husseyHiddenInvalidity152020]. Furthermore, applied psychological researchers seem to overlook the assumptions underlying their chosen method for evaluating reliability. For example, while Cronbach's $\alpha$ is always almost reported [@flakeStrengtheningFoundationEducational2021] and exceptionally popular [@mcneishThanksCoefficientAlpha2018], it is unclear whether researchers are aware of its underlying assumptions (that true scores and item variances should be equal and that item residuals should be uncorrelated, i.e., $\tau$ equivalence), and thus, limits its usefulness [@sijtsmaUseMisuseVery2008].

With this in mind, this oversight in measurement quality can raise reasonable concerns about the replicability of research findings because measurement contributes to the (in)validity of the findings [@flakeMeasurementSchmeasurementQuestionable2020; @lilienfeldPsychologicalMeasurementReplication2020]. Measurement oversight can be identified in decisions that researchers make, which potentially compromise the validity of the measures used in their studies, a phenomenon known as "*questionable measurement practices*" [@flakeMeasurementSchmeasurementQuestionable2020]. Further, the lack of evidence for measurement validity contributes to the difficulty in reaching a consensus about the best way to measure a construct. In the long run, it becomes a major obstacle to establishing cumulative psychological science [@elsonPsychologicalMeasuresAren2023]. Given these circumstances, it is critical to provide applied psychological researchers with a set of tools and practical steps to assist them in evaluating measurement validity so that they can improve their measurement practices, and thus, the credibility of their findings.

In this article, we aim to gently introduce the graded response model (GRM), which is a family of item response models, specifically aimed to assess the measurement precision of a polytomous (Likert-style) scale. We start this paper by briefly explaining the basics of Item Response Theory (IRT), the theoretical foundation of GRM, as well as contrasting its differences with the widely known Classical True Score Theory (CTT). In this paper, we aim to keep our tutorial concise so we do not cover all IRT concepts in detail. Interested readers are referred to available didactic texts on IRT, such as @embretsonItemResponseTheory2000 for the introductory level, and @bakerBasicsItemResponse2017 and @deayalaTheoryPracticeItem2022 for a more technical and comprehensive overview.

Further, we briefly explain what GRM is and how a GRM model is specified. In this part, we highlight how model and item parameters are specified and interpreted as well as visualizations (i.e., plots) derived from a GRM analysis. We also introduce three key assumptions underlying a GRM and how to test these assumptions. Subsequently, we provide an illustrative example of testing measurement precision using GRM, and in this part, we show how to practically implement GRM analysis and interpret its results. We complement the illustrative example with code in the open-source programming language R [@rcoreteamLanguageEnvironmentStatistical2023] so that the readers can implement and reproduce the example presented in this paper. Readers with some experience with R and RStudio can easily follow this tutorial, but for those who are not yet familiar with R, we refer them to excellent, easy-to-follow materials developed by @navarroPsychologicalScience2018. At last, we present some concluding remarks to encourage researchers to implement a GRM analysis as a part of their routine.

# A Brief Overview of Item Response Theory

To scrutinize their measurement quality, psychological researchers have relied upon two foundational principles underlying psychological testing, i.e., CTT and IRT, to decompose observed score into its *deterministic* and *random* or stochastic elements [@zumboValidityFoundationalIssues2006]. On the one hand, CTT decomposes observed score into *true* score and its *error* components. In practice, researchers implement this principle by conducting Structural Equation Modeling (SEM), Exploratory Factor Analysis (EFA), or Confirmatory Factor Analysis (CFA). Specifying the measurement part of SEM, or employing EFA and CFA models, allows for the decomposition of the covariance matrix of a given data into latent factors, which represent true score or actual ability or trait level, and error terms (i.e., residuals or unique variances). Consequently, the relationship between observed score and true score is *linear*, as true score is essentially a linear transformation of observed score [@embretsonItemResponseTheory2000]. IRT, on the other hand, models this relationship in a *probabilistic* rather than a linear fashion. More sharply, an IRT model assumes a probabilistic relationship between the observed score and the latent trait being measured ($\Theta$), and in particular, this probability takes into account the properties of the items, such as item discrimination, difficulty, and, if necessary, guessing [@bakerBasicsItemResponse2017].

The most critical distinction between CTT and IRT is how they conceptualize measurement error. In CTT, standard deviation of the observed score (*s*) is used to calculate standard error of measurement. Since measurement error relies on *s*, CTT assumes that measurement error is sample-dependent and constant for all individuals in the sample regardless of their trait level. This shortcoming restricts the utility of CTT, especially since psychologists often need to interpret individual scores, not just evaluate the test as a whole. Additionally, since standard error of measurement is also a function of reliability, which is typically denoted as a correlation between two parallel tests, reliability is presumed to be equal across different levels of a trait or ability. This is unrealistic since researchers frequently encounter situations where a test is excessively difficult for a group of low-performing participants, providing little information beyond indicating that their trait or ability level is significantly lower than what the test can measure.

IRT offers an elegant solution to this issue by enabling the calculation of standard errors for each individual (*SE*~$\theta$~), and test reliability can be inferred from the average of *SE*~$\theta$~ across a sample of participants [@embretsonItemResponseTheory2000; @langSciencePracticeItem2021]. This approach allows for the estimation of reliability at varying levels of trait or ability (i.e., test information function - TIF). By doing this, when a test is overly challenging for low-performing individuals or too easy for high-performing individuals, IRT analysis can help identify the levels of trait or ability where the test is most reliable. A concrete example of this is a study scrutinizing the reliability of the Short Dark Tetrad (SD4) scale from an IRT perspective [@blotnerDarkTriadDead2022]. According to this study, the sadism subscale of the SD4 scale is the most reliable for measuring individuals with average to high levels ($\bar{x}$ \< $\Theta$ \< 2.5 *SD*) of sadism but suboptimal for measuring those with low levels of sadism [@blotnerDarkTriadDead2022]. Therefore, IRT models offer a more informative and nuanced perspective, proving useful for researchers who wish to closely examine the performance of their measures.

Further, IRT allows researchers to examine the performance of a specific item (hence "item" response theory) by specifying the relationship between item score and $\Theta$ given one-, two-, or three-parameter. One-parameter logistic (1PL) or Rasch model, the simplest variant of the IRT models, accounts only item difficulty (*b*) while presuming equal item discrimination (*a*) across items. Practically, this model slightly overlaps with CTT in the sense that they assume that all items carry equal informative value thus the estimated $\Theta$ of a 1PL/Rasch model is identical to the sum score of CTT [@langSciencePracticeItem2021; @stemlerRaschMeasurementItem2021]. This notion actually stems from a philosophical principle of *specific objectivity*, which requires that comparisons between measurements remain independent of both the item and individual characteristics [@raschSpecificObjectivityAttempt1977]. Put simply, Rasch model assumes that the latent trait ($\Theta$) should remain unaffected by specific items used in the test. For example, the difference in depression levels between two individuals should be always the same regardless of the scale used to measure their depression levels. While this principle reasonably enforces objectivity in measurement practices, it demands a strong theory delineating the construct and strict requirements of data-model fit while both assumptions rarely hold in a real-world scenario, especially in psychological science.

Furthermore, the two-parameter logistic (2PL) model fills this gap by allowing item ability to differentiate individuals with varying levels of $\Theta$ (i.e., item discrimination parameter - *a*) to differ. Additionally, in some contexts, researchers may suspect that a part of the probabilistic relationship between observed score and $\Theta$ is explained by guessing thus three-parameter logistic (3PL) model incorporates guessing parameter (*c*). The field of IRT has rapidly developed to include various models suited to specific contexts, such as handling ordinal responses [@murakiGeneralizedPartialCredit1992; @samejimaGradedResponseModel1997], categorical responses [@thissenNominalCategoriesItem2013], or assessing multidimensional traits simultaneously [@bockMarginalMaximumLikelihood1981; @chalmersMirtMultidimensionalItem2012]. It is important to note that 1PL, 2PL, and 3PL models are only applicable to binary or dichotomous data (e.g., true/false response), and the focus of this article is nonetheless to show the utility of IRT for fitting the ordered (Likert-style) responses. We briefly summarize the features of the most frequently employed IRT models in {apatb-table1}.

# Graded Response Model

The GRM is a family of IRT models specifically designed to analyze ordered polytomous (Likert-style) data [@samejimaGradedResponseModel1997; @samejimaGeneralGradedResponse2010; @samejimaGradedResponseModels2016]. The fundamental idea of GRM is to extend the logic of simpler dichotomous IRT models to polytomous scales by applying a probabilistic function to each response category. In dichotomous 2PL IRT models, the probability of answering a correct response is modeled as a function of $\Theta$, item difficulty, and item discrimination. GRM extends this idea by modeling the probability of picking a certain response category or higher on an item (i.e., *step function*) given to item difficulty and discrimination. For each response category (e.g., answering "agree" on a five-point Likert scale), GRM calculates the cumulative probability of a participant answering "agree" or above, given to their $\Theta$. Therefore, the notion of item difficulty in dichotomous IRT models is extended to the step function (i.e., item threshold) in order to handle ordered polytomous data.

To illustrate the step function, consider a scale measuring sadistic personality (e.g., "*watching a fist-fight excites me*," etc.) with response categories ranging from 1 (*strongly disagree*) to 4 (*strongly agree*). Imagine these categories are hierarchically ordered from the lowest category (*strongly disagree*) to the highest (*strongly agree*), like a staircase. GRM aims to estimate the level of sadistic personality ($\Theta$) required for "stepping" from one response category to another (e.g., *moderately agree* to *strongly agree*). Therefore, a GRM model of a five-point Likert scale calculates four item threshold parameters (*b*): the location of $\Theta$ level where individuals are equally likely to respond 1 or 2 (*b*~1~), 2 or 3 (*b*~2~), 3 or 4 (*b*~3~), and 4 or 5 (*b*~4~). Each threshold is exactly the point where a participant is equally likely to respond to either of the adjacent response categories. For example, if *b*~1~ = -1.23 for the item "*watching a fist-fight excites me*", it implies that participants with sadism level 1.23 below the mean are equally likely to either answer *strongly disagree* or *moderately disagree*.

Let's take a closer look at {apafg-fig1}, which shows an example of an item probability function (IPF) from an item of a scale with five response categories. In general, IPF describes a relationship between $\Theta$ (x-axis) and the probability of endorsing a response category (y-axis - *P*$\Theta$). {apafg-fig1} consists of five category probability curves, each of which represents the probability of endorsing a response category given $\Theta$. Item threshold is exactly the location of $\Theta$ where two adjacent category probability curves cross each other.

## Assumptions

Before running an analysis, researchers need to be aware of assumption underlying a GRM model. The first key assumption is that $\Theta$ is monotonically correlated with the probability of endorsing a response category. In other words, individuals with higher levels of $\Theta$ have a higher probability of endorsing response category that represents greater intensity of the traits [@hambletonIRTModelsAnalysis2010]. For instance, people with higher tendency of sadistic personality have a greater chance of answering "*moderately agree*" or "*strongly agree*" on a sadism scale compared to those with lower levels of sadism.

The second key assumption pertains to unidimensionality of $\Theta$. This implies that the item pool tested using a GRM analysis should represent only one latent trait ($\Theta$). Although it is indeed possible to account for multiple $\Theta$ in a IRT model (multidimensional IRT, see @bockFullInformationItemFactor1988, we limit the scope of our tutorial to the unidimensional GRM. To test whether an item pool represents only one $\Theta$, researchers can apply a parallel analysis [@guoAssessingDimensionalityIRT2023] or a factor analysis [@hambletonAssessingDimensionalitySet1986], before specifying their GRM models.

Relatedly, a GRM model assumes that the item pool are locally independent, as the third assumption. Local independence suggests that participants' responses to one item do not influence their response to another. It is important to note that evidence for unidimensionality does not always warrant local independence. As an eyeball example - consider a trust in science scale containing these two items: "*I trust scientists working in the natural science*" and "*most biologists are trustworthy*." While both items might measure the same latent trait (i.e., trust in science), those who trust scientists working in the natural science are very likely to also trust biologists. To inspect whether items are locally independent, researchers can examine the relationships between item responses after accounting for $\Theta$ using residual correlation analysis [@chenLocalDependenceIndexes1997].

# An Illustrative Example of Graded Response Model: The Right Wing Authoritarianism (RWA) Scale

## A Brief Overview of the Altemeyer's RWA Scale

To illustrate the application of GRM for analyzing measurement precision of an ordered polytomous scale, we provide an example of a GRM analysis of the RWA scale data [@altemeyerRightwingAuthoritarianism1981; @altemeyerAuthoritarians2006]. @altemeyerRightwingAuthoritarianism1981 defines RWA as a personality propensity to blindly abide established authorities (i.e., authoritarian submission), to act aggressively toward individuals who are perceived to be punished by these established authorities (i.e., authoritarian aggression), and to uphold traditional values promoted by the authorities (i.e., conventionalism). Individuals with higher levels of RWA tend to view the world as a dangerous place [@duckittDualProcessMotivational2017], and thus, to ensure stability and safety, they are motivated to preserve social order by deferring to those whom they perceive as legal, social, or moral authorities; for instance, the government, religious institutions, or political or religious or military leaders [@saundersRightWingAuthoritarianismScale2017]. The RWA scale developed by @altemeyerRightwingAuthoritarianism1981 remains as one of the most important measures in the field of social and political psychology [@saundersRightWingAuthoritarianismScale2017], and has helped researchers to conceptualize the role of RWA in shaping various social psychological outcomes, such as prejudice, political behavior, and various antisocial behaviors @akramiRightWingAuthoritarianismSocial2006; [@osbornePsychologicalCausesSocietal2023; @sibleyPersonalityPoliticalOrientation2012].

In this paper, we use the latest version of the RWA scale [@altemeyerAuthoritarians2006], which consists of 22 items that participants are asked to respond their agreement to those items on a nine-point scale, ranging from "*strongly disagree*" (-4) to "*strongly agree*" (+4). While the RWA scale consists of three sub-dimensions, i.e., submissiveness, aggression, and conventionalism, the RWA scale is (theoretically) unidimensional because [@altemeyerAuthoritarians2006] shows that the sub-dimensions are strongly intercorrelated. While some evidence shows otherwise [@duckittMultidimensionalityRightWingAuthoritarian2013], for the purpose of this article, we assume that the RWA scale is unidimensional.

# Disclosure and Data Availability Statements

To maximize the reproducibility of our illustrative example, we wrote the article as a Quarto (.qmd) document, where we integrate the R codes used in the analysis as well as its outputs. We also include the annotated R script file (.R) for the example we used as a supplementary document, which we highly recommend to use for a didactic purpose. The Quarto file (and its corresponding .docx and .pdf output) and R script file are publicly available on [a Github repository](https://github.com/rameliaz/grm-tutorial-paper). The dataset we used in this tutorial paper was obtained from [Open-Source Psychometrics Project](http://openpsychometrics.org/_rawdata/), which is publicly accessible.

## Step 1: Preparation

```{r}
install.packages("tidyverse", "psych", "devtools", "mirt", "caret", dependencies=TRUE)
devtools::install_github("masurp/ggmirt") # remote installation through GitHub repository
```

Blabla

```{r}
library(ggmirt); library(tidyverse); library(psych); library(mirt); library(caret)
```

Blabla

```{r}
ds <- read.csv("data/data.csv")
```

Blabla

```{r}
str(ds)
```

Blabla

```{r}
rwa <- subset(ds, select = Q1:Q22)
str(rwa)
```

Blabla

## Step 2: Inspecting Key Descriptive Statistics

```{r}
psych::describe(rwa)
```

As we see in {apatb-table2}, Kok ada nilai 0-nya? padahal kan skor minimalnya 1. Coba kita hitung frekuensi nilai 0 di tiap item.

```{r}
zero <- colSums(rwa == 0) / nrow(rwa) * 100 # Computing the frequency of "0" in each column.
print(zero) # The proportion of "0" for each item.
```

Kita anggap nilai 0 adalah NA dan kita delete semua case dengan nilai NA

```{r}
rwa <- rwa %>%
  mutate_all(~na_if(., 0)) %>%  # Replacing 0 with NA in all columns.
  drop_na()  # Removing cases with any NA values.
```

Kalau lihat di {apatb-table2}, meannya kok beda2? oh iya karena ada unfavorable items. Ayo kita reverse score dulu ya.

```{r}
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
rwa <- rwa %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) + 1 to reverse code the unfavorable items.
```

## Step 3: Examining Dimensionality

```{r}
#| eval: false

irt.fa(rwa, nfactors = 1, fm = "minres")
```

lalalala

```{r}
#| eval: false

fa.parallel(rwa, nfactors = 1, fm="minres", fa="fa", cor = "poly")
```

lalalalal

```{r}
cor <- cor(rwa,method="pearson") # First, creating a (pearson) correlation matrix.
efa <- fa(rwa, nfactors=1, fm="minres") # Now, running exploratory factor analysis. 
print(efa) # Print the results.
```

Scree plot

Now lets do parallel analysis

```{r}
pa <- fa.parallel(rwa, fm="minres", fa="fa")
```

lalalala

```{r}
pa$fa.values
```

Eigenvalues `12.225/0.843`

## Step 4: Model Estimation, Parameters, and Fit Statistics

```{r}
model <- 'rwa = 1-22'
```

lalalala

```{r}
fit <- mirt(data=rwa, 1, model=model, itemtype="graded", SE=T, verbose=F)
```

llalalala

```{r}
coefs <- coef(fit, IRTpars=T, simplify=T) # Storing model parameters in a data frame.
print(coefs) # Yielding model parameters: item discrimination (a) and threshold (b).
```

lalalala

```{r}
summary(fit) 
```

bla bla lhalalala

```{r}
M2(fit, type="C2")
```

lhalalala

```{r}
#| include: false

m2_results <- M2(fit, type="C2")
m2_results[] <- lapply(m2_results, as.numeric)
m2_stat <- m2_results$M2
m2_stat = round(m2_stat, digits = 3)
df <- m2_results$df
p_value <- m2_results$p.value
rmsea <- m2_results$RMSEA
rmsea = round(rmsea, digits=3)
srmsr <- m2_results$SRMSR
srmsr = round(srmsr, digits=3)
tli <- m2_results$TLI
tli = round(tli, digits=3)
cfi <- m2_results$CFI
cfi = round(cfi, digits=3)
```

According to our analysis, the model does not fit the data well (*M*(`r df`) = `r m2_stat`, *p* = `r p_value`, *RMSEA* = `r rmsea`, )

```{r}
item.fit <- itemfit(fit)
```

## Step 5: Model Residuals

```{r}
ld <- residuals(fit, type = "LD") # Running local dependency statistics
up <- which(upper.tri(ld), arr.ind = T) # Extracting values only on the upper side of the diagonal.
lar <- up[ld[up] > 0.2 | ld[up] < -0.2, ] # Defining unusually large residuals (>0.2).
```

lhalalala

```{r}
#| eval: false

for (i in 1:nrow(lar)) {
  row <- lar[i, 1]
  col <- lar[i, 2]
  value <- ld[row, col]
  cat(sprintf("A large residual correlation is found between item %d and item %d: %f\n", row, col, value))
} # Now we detect the problematic pairs.
```

lhalalala

```{r}
q3 <- residuals(fit, type = "Q3") # Running Yen's Q3 statistics
findCorrelation(q3, cutoff = 0.2, verbose = T) # Detecting problematic correlation pairs.
```

## Step 6: IRT Plots

Now spit out the

```{r}
tracePlot(fit, facet=T, title = "Category Probability Functions of RWA Scale") + labs(color="Response Options")
```

lhalalala

```{r}
itemInfoPlot(fit, facet=T, title = "Item Information Curves of the RWA Scale")
```

lhalalala

```{r}
testInfoPlot(fit, title="Test Information Curve of the RWA Scale")
```

## Step 7: Computing Reliability

```{r}
m_rel <- marginal_rxx(fit)
```

lhalalala

```{r}
conRelPlot(fit, title="Reliability of the RWA Scale Given to the θ Level")
```

lalalala

```{r}
theta_se <- fscores(fit, full.scores.SE = T) # Extracting the estimated theta score of each participant.
e_rel <- empirical_rxx(theta_se) # Then use the estimated theta to calculate empirical reliability.
```

lhalalala

```{r}
omega(rwa)
```

# Conclusions

Since construct validation is an ongoing process [@messickValidityPsychologicalAssessment1995], applied researchers can contribute to the cumulative science by reporting evidence for measurement validity [@floraPurposePracticeExploratory2017], and this can be done by implementing and reporting a GRM analysis.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

```{r}
#| label: apatb-table1
#| apa-cap: Comparison Between Common IRT Models
#| apa-note: NULL
#| echo: false
```

| Model                                   | Key Characteristics                                                                                                                                                                                                                                          | Data Type   | Response Options        |
|----------|--------------------------------------------|----------|----------|
| 1-PL Model (Rasch Model)                | 1\. Estimates only item difficulties (*b*). 2. Assumes that all items have equal discrimination parameters (*a*). 3. Item and person parameters are independent.                                                                                             | Dichotomous | Correct/Incorrect (0/1) |
| 2-PL Model                              | 1\. Estimates item difficulties (*b*) and item discrimination (*a*). 2. Less stringent than 1-PL model since it allows item discrimination parameters (*a*) to vary.                                                                                         | Dichotomous | Correct/Incorrect (0/1) |
| 3-PL Model                              | 1\. Estimates item difficulties (*b*), discrimination (*a*), and pseudo-guessing parameter (*c*). 2. Appropriate for modeling a test data with multiple responses (e.g., multiple-choice tests), and thus, guessing might influence participants' responses. | Dichotomous | Correct/Incorrect (0/1) |
| Graded Response Model (GRM)             | 1\. Appropriate for modeling ordinal data with more than two response categories (i.e., Likert-style). 2. Estimates a discrimination parameter (*a*) and multiple threshold parameters (*b*) per item.                                                       | Polytomous  | Ordered Categories      |
| Partial Credit Model (PCM)              | 1\. An extension of the 1-PL (Rasch) model for polytomous items. 2. Estimates thresholds between adjacent categories but assumes equal discrimination across items.                                                                                          | Polytomous  | Ordered Categories      |
| Generalized Partial Credit Model (GPCM) | Extending PCM to allow differential discrimination parameters across items.                                                                                                                                                                                  | Polytomous  | Ordered Categories      |
| Nominal Response Model (NRM)            | 1\. Appropriate for modeling categorical responses with no order. 2. Estimates discrimination parameters (*a*) and multiple category-specific parameters (*b*).                                                                                              | Categorical | Unordered Categories    |

{{< pagebreak >}}

```{r}
#| label: apafg-fig1
#| apa-cap: An Item Probability Function from a GRM Model
#| apa-note: NULL
#| output: true
#| echo: false

knitr::include_graphics("CPF example.png")
```

{{< pagebreak >}}

```{r apatb-table2}
#| output: true
#| echo: false
#| apa-cap: Descriptive Statistics of RWA Scale
#| apa-note: SD = Standard Deviation
#| ft.align: left

# Transforming describe() output into a APA-formatted table
rwa <- subset(ds, select = Q1:Q22)
table1 <- psych::describe(rwa); table1 <- as.data.frame(table1)
table1 <- table1 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, mean, sd, min, max, range, n) %>% 
  rename(Mean=mean, SD=sd, Minimum=min, Maximum=max, Range=range) %>% 
  flextable() %>%
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table1 <- colformat_double(x=table1, j="Minimum", digits = 0)
table1 <- colformat_double(x=table1, j="Maximum", digits = 0)
table1 <- colformat_double(x=table1, j="Range", digits = 0)
table1 <- colformat_double(x=table1, j="n", digits = 0)
table1
```

{{< pagebreak >}}

```{r apatb-table3}
#| output: true
#| echo: false
#| apa-cap: Descriptive Statistics of RWA Scale
#| apa-note: Descriptive Statistics After Reversing Unfavorable Items and removing cases with NA. SD = Standard Deviation.
#| ft.align: left

rwa <- rwa %>%
  mutate_all(~na_if(., 0)) %>%  # Replacing 0 with NA in all columns.
  drop_na()  # Removing cases with any NA values.
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
rwa <- rwa %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) to reverse code the unfavorable items.
# Transforming describe() output into a APA-formatted table
table2 <- psych::describe(rwa); table2 <- as.data.frame(table2)
table2 <- table2 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, mean, sd, min, max, range, n) %>% 
  rename(Mean=mean, SD=sd, Minimum=min, Maximum=max, Range=range) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table2 <- colformat_double(x=table2, j="Minimum", digits = 0)
table2 <- colformat_double(x=table2, j="Maximum", digits = 0)
table2 <- colformat_double(x=table2, j="Range", digits = 0)
table2 <- colformat_double(x=table2, j="n", digits = 0)
table2
```

{{< pagebreak >}}

```{r fig2}
#| output: true
#| echo: false
#| label: apafg-fig2
#| apa-cap: Scree Plot
#| apa-note: NULL

plot(efa$values, type = "b", main = "Scree Plot", xlab = "Factor", ylab = "Eigenvalue") # Scree plot
abline(h = 1, col = "red", lty = 2) # Add new line to factor 1.

```

{{< pagebreak >}}

```{r fig3}
#| output: true
#| echo: false
#| label: apafg-fig3
#| apa-cap: Parallel Analysis
#| apa-note: NULL

plot(pa)
```

{{< pagebreak >}}

```{r}
#| include: false

sum <- summary(fit) 
```

```{r apatb-table4}
#| output: true
#| echo: false
#| apa-cap: Item Parameters
#| apa-note: λ = Standardized Factor Loadings, h2 = Commonality, α = Discrimination, β1.4 = Response specific difficulty parameters (item threshold).
#| ft.align: left
#| column: page

table3 <- as.data.frame(coefs$items)
load <- as.data.frame(sum$rotF)
h2 <- as.data.frame(sum$h2)
table3 <- cbind(table3, load, h2)
table3 <- table3 %>% 
  rownames_to_column(var="Item") %>% 
  select(Item, a, b1, b2, b3, b4, b5, b6, b7, b8, rwa, h2) %>% 
  rename(α=a, β1=b1, β2=b2, β3=b3, β4=b4, β5=b5, β6=b6, β7=b7, β8=b8, λ=rwa) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table3 <- colformat_double(x=table3, j="α", digits = 2)
table3 <- colformat_double(x=table3, j="β1", digits = 2)
table3 <- colformat_double(x=table3, j="β2", digits = 2)
table3 <- colformat_double(x=table3, j="β3", digits = 2)
table3 <- colformat_double(x=table3, j="β4", digits = 2)
table3 <- colformat_double(x=table3, j="β5", digits = 2)
table3 <- colformat_double(x=table3, j="β6", digits = 2)
table3 <- colformat_double(x=table3, j="β7", digits = 2)
table3 <- colformat_double(x=table3, j="β8", digits = 2)
table3

```

{{< pagebreak >}}

```{r apatb-table5}
#| output: true
#| echo: false
#| apa-cap: Item Fit Statistics
#| apa-note: Scaled χ2 Statistics. RMSEA = root mean square error of approximation, CFI = comparative fit index, TLI = Tucker-Lewis index, SRMR = Standardized Root Mean Square Residual.
#| ft.align: left

table4 <- as.data.frame(item.fit)
table4 <- table4 %>%
  select(item, S_X2, df.S_X2, RMSEA.S_X2, p.S_X2) %>% 
  rename(Item=item, χ2=S_X2, df=df.S_X2, RMSEA=RMSEA.S_X2, p=p.S_X2) %>% 
  flextable() %>% 
  theme_apa() %>% 
  line_spacing(part = "all") %>% 
  padding(padding.top = 5, padding.bottom = 5)
table4 <- colformat_double(x=table4, j="χ2", digits = 2)
table4 <- colformat_double(x=table4, j="df", digits = 2)
table4 <- colformat_double(x=table4, j="RMSEA", digits = 2)
table4 <- colformat_double(x=table4, j="p", digits = 2)
table4

```

{{< pagebreak >}}

```{r fig4}
#| output: true
#| echo: false
#| label: apafg-fig4
#| apa-cap: Item Probability Functions of RWA Scale
#| apa-note: NULL

tracePlot(fit, facet=T, title = "Item Probability Functions of RWA Scale") + labs(color="Response Options")
```

{{< pagebreak >}}

```{r fig5}
#| output: true
#| echo: false
#| label: apafg-fig5
#| apa-cap: Item Information Curves of the RWA Scale
#| apa-note: NULL

itemInfoPlot(fit, facet=T, title = "Item Information Curves of the RWA Scale")
```

{{< pagebreak >}}

```{r fig6}
#| output: true
#| echo: false
#| label: apafg-fig6
#| apa-cap: Test Information Curve of the RWA Scale
#| apa-note: NULL

testInfoPlot(fit, title="Test Information Curve of the RWA Scale")
```

{{< pagebreak >}}

```{r fig7}
#| output: true
#| echo: false
#| label: apafg-fig7
#| apa-cap: Reliability of the RWA Scale Given to the θ Level
#| apa-note: NULL

conRelPlot(fit, title="Reliability of the RWA Scale Given to the θ Level")
```
