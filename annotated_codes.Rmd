---
title: "Tutorial on running a Graded Response Model"
author: "Rizqy Amelia Zein & Muhamad Hanif Akhtar"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Welcome to the tutorial!

This tutorial aims to familiarize applied psychological researchers to item response theory (IRT) modeling, or more specifically, a graded response model (GRM). GRM is a part of IRT family, specifically designed for examining the performance of a scale with polytomous responses (Likert-type scale).

To this end, in ***the first part***, we will show you how to run a GRM, using data from the Open Psychometric Database, specifically the right-wing authoritarianism (RWA) scale (Altemeyer). In ***the second part***, we will show you how to run a GRM using a simulated dataset.

To optimally use this material, we strongly recommend to go through the first part (RWA) and then the second part (simulated data).

## THE FIRST PART: RWA Scale

### Step 1a: Preparation - Install and activate the packages

First of all, we should install the packages we need for the analysis. If you already have these packages already installed in your device, you can safely skip this step.

```{r, include=FALSE}
install.packages("tidyverse", # for data wrangling
"psych", # for descriptive statistics and unidimensionality test
"devtools", # for installing R package that is not available on CRAN
"mirt", # for conducting the main GRM analysis
"caret", # for helping us detecting large residuals correlation (local independence test)
dependencies=TRUE)
```

Run the above codes only if you have not had them installed.

Package `ggmirt` is not available in CRAN, so we have to do a remote installation by downloading the package from its Github repository.

```{r, include=FALSE}
devtools::install_github("masurp/ggmirt") # remote ggmirt installation through GitHub repository
devtools::install_github("masurp/ggmirt")
```

Now, we activate the package.

```{r, include=FALSE}
library(tidyverse); library(psych); library(mirt); library(ggmirt); library(caret)
```

### Step 1b: Preparation - Downloading, storing, and preparing the data for analysis

Before running our analysis, of course we should download our dataset first. Here, we have written codes to automatically download the file from the Open Psychometric database. You can always adjust these codes according to your needs.

```{r}
url <- "https://openpsychometrics.org/_rawdata/RWAS.zip" # Defining the URL of the zip file
```

If you look closely at the .zip folder, there are two files in there; one is a `.csv` file (this is the dataset), and two, a `.txt` file (a codebook). Since we need them both, we will extract them and make them readable in R environment.

```{r}
zip <- tempfile(fileext = ".zip") # Defining the path to temporarily save the downloaded zip file
download.file(url, zip, mode = "wb") # Downloading the zip file
do <- tempdir() # Defining the path to extract the contents of the zip file
unzip(zip, exdir = do) # Unzipping the .zip file
files <- list.files(do, recursive = TRUE, full.names = TRUE) # List all the files in the extracted directory, including subdirectories

csv <- grep("\\.csv$", files, value = TRUE) # Filtering for .csv files
txt <- grep("\\.txt$", files, value = TRUE) # Filtering for .txt files


if (length(csv) == 0) {
  stop("no .csv found")
} # now checking if there are any .csv files, and if that's the case, the script gives no error message

if (length(txt) == 0) {
  stop("no .txt found")
} # do the same thing for .txt file

ds <- read.csv(csv[1]) # Importing the .csv file.
codebook <- readLines(txt[1]) # Importing the .txt file.

unlink(zip)
unlink(do, recursive = TRUE)
rm(csv, txt, do, files, url, zip)
# This three lines of script are used to clean up the temporary files and values remaining in the R environment

str(ds) # Now we are checking the structure of data frame "ds", which contains our data so that we can work with it!
```

By looking at the structure, it's very likely that columns "Q1-Q22" are RWA items, because RWA consists of 22 items. but let's check the code book just to make sure...

```{r}
print(codebook)
```

If we look at the codebook, columns "Q1-Q22" indeed are the RWA data (see line 5-7), so our next step is to create a new data frame that contains only RWA items because this what we are interested in.

```{r}
data <- subset(ds, select = Q1:Q22) # now we subset our data because we are only interested in RWA items.
rm(codebook) # now we delete the codebook since we no longer need it.
```

After executing this line of code above, we shall see in the R environment that we have a new data frame, namely `data`, which contains of 22 columns (variable) and 9881 data points (participants).

Now let's check the structure of our new data frame containing only RWA items.

```{r}
str(data)
```

The structure seems correct, so we can proceed to the next step.

### Step 2: Inspecting key descriptive statistics

This part is fairly straightforward. Before performing analysis, it is very important to explore key descriptive statistics of the data. We are using a neat function in `psych` package: (`describe()`)

```{r}
describe(data)
```

Why there are `0` values in all columns (see column `min`)? The items are not supposed to have `0` value because the responses are ranging from 1-9.

Let's count how many `0` we have in our data.

```{r}
zero <- colSums(data == 0) / nrow(data) * 100 # Computing the percentage of "0" in each column.
print(zero) # Showing the percentage of "0" for each item.
rm(zero) # Removing the unused vector
```

We have a few cases with `0` in all items. There is no explanation what `0` means in the codebook, but it is very likely that `0` is used to code a missing response. It is actually still possible to run an IRT analysis with missing data, but we have to ensure that the data are missing at random (MAR). One solution for this is simply exclude all cases with missing responses.

Let's assume then that `0` means missing response, and then delete all cases with missing responses.

```{r}
data <- data %>%
  mutate_all(~na_if(., 0)) %>%  # Replacing 0 with NA in all columns.
  drop_na()  # Removing cases with any NA values.

describe(data)
```

As we see here, mean score of the items differ - some items have small mean scores (\~2-3), while the others are quite large (\~6-7). The reason for this is that the RWA scale has unfavorable items so to simplify the interpretation, we should reverse their scores.

Unfavorable items are Q4, Q6, Q8, Q9, Q11, Q13, Q15, Q18, Q20, and Q21

```{r}
unfav <- c("Q4","Q6","Q8","Q9","Q11","Q13","Q15","Q18","Q20","Q21") # Now we create a vector defining which items will be coded reversely.
data <- data %>% 
  mutate(across(all_of(unfav), ~ 10 - .))# We simply subtract the scores from 9 (the maximum) + 1 
# to reverse code the unfavorable items.
rm(unfav) # We don't need the vector, so we remove it now to keep our workspace clean.
```

Then let's check our data again.

```{r}
describe(data)
```

Now all cases with missing responses have been excluded, and unfavorable items have been reverse scored but we lost a few proportion of our sample. Losing some cases is indeed a downside of our decision, but we since it is unclear what `0` means (it is safer to assume that it a code for missing responses) and we still have a sizeable sample (\>9000), let's proceed to the next step!

### Step 3: Examining dimensionality

While now it is computationally possible to run a multidimensional IRT analysis, in general, an IRT model assumes that the scale is unidimensional. Bob Altemeyer (1996) argues in his book that while the RWA scale consists of three sub-dimensions, which are submissiveness, aggression, and conventionalism, the RWA scale is essentially unidimensional because these three sub-dimensions are strongly intercorrelated. Therefore, we may assume that the scale is supposed to be unidimensional.

To check the unidimensionality of the RWA scale, we can implement two approaches:

First, we can run an exploratory factor analysis (EFA) using a polychoric correlation matrix. This approach is generally recommended if we have a Likert scale with 4-7 possible responses. This can be neatly done using a simple script (`psych`) below:

```{r}
irt.fa(data, nfactors = 1, fm = "minres")
```

The option `fm = "minres"` implies that we choose minimum residual as a factoring method due to its effectiveness in handling non-normally distributed data. However, as we see in the console, there is an error message telling us that R stopped the analysis. This is because performing parallel analysis or EFA using a polychoric correlation matrix is not needed when we have more than 8 categories (responses) in our scale. This means we can treat our data as continuous (instead of ordinal). Since the RWA scale has nine responses, let's do EFA and parallel analysis manually using a correlation matrix instead.

Before running an EFA, we need to check whether our data is normally distributed by calling this command:

```{r}
skim(data)
```

As seen in the console, the function creates an intuitive visualization of data distribution (look at `hist` column in the right), which shows that our data is heavily skewed. Therefore, we could use a Spearman's correlation matrix as an input for our EFA.

```{r}
cor <- cor(data, method="spearman") # First, creating a (spearman) correlation matrix.
efa <- fa(cor, nfactors=1, fm="minres") # Now, running exploratory factor analysis. 
print(efa) # Print the results.
```

Let's look at the output.

MR1 reflects factor loadings of each item. As we see, all items are strongly loaded to the single factor. The rule of thumb is `0.3`, and we can see here all the loadings are more than `0.3`. We also see that the proportion of total variance that is explained by the factor (see `Proportion Var`) shows `0.56`, this means that the factor accounts for a significant portion (56%) of the variance of the data. In general, our assumption regarding the unidimensionality of the RWA scale holds, but to paint a clearer picture, let's look at the scree plot.

```{r}
plot(efa$values, type = "b", main = "Scree Plot", xlab = "Factor", ylab = "Eigenvalue") # Scree plot
abline(h = 1, col = "red", lty = 2) # Add new line to eigenvalue = 1.
```

The first line of code is a function to plot the eigenvalues (`efa$values`, x-axis) given the number of latent factors (y-axis). The option `type = "b"` implies that we ask R to plot the eigenvalues at exact points and a line connecting these points along the number of factors. The second line of code aims to display a horizontal red line (i.e., a reference line) to see which factors have eigenvalues greater than one, which is a common criterion for factor retention.

As we see in the plot, the eigenvalue significantly levels off after one factor, which further substantiates our assumption.

In a case where we are unsure about the number of possible factors underlying the data, we can run a parallel analysis also with a polychoric correlation matrix. This approach is, in general, better than EFA because a parallel analysis compares eigenvalues (a scalar which measures the amount of variance that is accounted for by each latent factor) from a set of simulated data with eigenvalues of our data. As recommended by @guoAssessingDimensionalityIRT2023, we will also use principal component analysis (`fa="pc"`). Again, to perform this, we need to run a simple command:

```{r}
fa.parallel(data, fm="minres", fa="pc")
```

Here, we use minimum residuals (`minres`) algorithm to extract/identify latent factors because it is considered suitable for dealing with non-normally distributed data. Parallel analysis shows that 2 components are suggested. Then what if we run EFA again, now with two latent factors as an assumption?

```{r}
efa <- fa(cor, nfactors=2, fm="minres") # Running exploratory factor analysis with 2 factors. 
print(efa) # Print the results.
```

### Step 4: Model estimation, parameters, and fit statistics

We have evidence that the scale is unidimensional, now it is the time to estimate our model. As the first step, let's define our model:

```{r}
model <- 'theta = 1-22' # This script means that our model consists of a latent factor namely theta
# and consists of 22 items (column 1 to column 22).
```

Now, we estimate our model...

```{r}
fit <- mirt(data=data, 1, model=model, itemtype="graded", SE=T, verbose=F) # This script means we're running a GRM analysis by assuming that the RWA scale as a unidimensional construct 
```

The number `1` indicates that we assume that the model contains only one $\theta$, i.e., unidimensional. We also set the model specification to `model = model`, which we have previously determined. We set `itemtype` to `"graded"` because we want to run a GRM model. The `SE` option is set to `TRUE` because we ask R to also estimate the standard error. At last, we set the `verbose` option to `FALSE` to avoid R displaying unnecessary information during the model fitting process, so the `mirt()` function can run quietly.

Now we store the model parameters in a data frame.

```{r}
coefs <- coef(fit, IRTpars=T, simplify=T) # Storing model parameters in a data frame.
```

The first line of code indicates that we are calling a `coef()` function to calculate item parameters from `fit` model, and then, store them in a new data frame called `coefs`. The `IRTpars` and `simplify` option is set to `TRUE` because we want to display the traditional IRT parameters (*a* and *b*) in a simple, readable format without showing the standard errors.

Let's take a closer look at the model parameters!

```{r}
print(coefs) # Yielding model parameters: item discrimination (a) and threshold (b).
```

Now we can see item discrimination (a), which indicates how well an item differentiates participant with different levels of authoritarian personality. To interpret this, we could use a guideline from Baker (2017). RWA items, in general, have a moderate to very high ability to differentiate participants with different theta levels. We can see that most items have 8 item threshold each (b1-b8), and this is because we have nine response options.

```{r}
summary(fit) # Displaying factor loadings and commonality.
```

Before estimating a graded response model, `mirt` ran an EFA, and now as we can see in the console, we are looking at the EFA results. The results are slightly different from our previous EFA analysis, because `mirt` runs EFA using a quasi polychoric correlation matrix, while the one we ran earlier used a pearson correlation matrix. However, most importantly, we see that all items are significantly loaded to one factor, and the factor now substantially accounts for 65.1% of the variance in the data, which strengthens our assumption that the RWA scale is unidimensional.

Now, let's look at the (model and item) fit statistics

```{r}
M2(fit, type="C2") # Running model fit analysis.
```

Apparently, our model, overall, does not fit well with the data. Could it be a problem of model misspecification?

```{r}
itemfit(fit) # Yielding item fit statistics.
```

Looking at RMSEA and p values of the scaled X2 statistics, we can see here that only Q2, Q4, and Q12 fit to our data.

Taking account the model and item fit statistics, it is very likely that we have problems with our model. It could be a problem of model misspecification, or something else. We can identify potential problems by looking at local dependency statistics (Step 6) and the plots (Step 7).

### Step 5: Probing model misspecification by examining residuals

While we have evidence that the RWA scale is unidimensional, our model does not fit well with the data. Therefore, it is reasonable to suspect that our model is locally dependent, thus violating IRT assumption.

We will then further examine the possibility of model misspecification by looking at model residuals. In this tutorial, we will introduce two ways to examine residuals. First, we will inspect a local dependence matrix derived from the X2 statistics (Chen & Thissen, 1997), and second, we will look at Yen's Q3 statistics.

```{r}
ld <- residuals(fit, type = "LD") # Running local dependency (LD) statistics.
up <- which(upper.tri(ld), arr.ind = T) # Creating a new matrix containing values only on the upper side of the diagonal (correlations).
lar <- up[ld[up] > 0.2 | ld[up] < -0.2, ] # A vector defining unusually large residuals ( > |0.2|).

for (i in 1:nrow(lar)) {
  row <- lar[i, 1]
  col <- lar[i, 2]
  value <- ld[row, col]
  cat(sprintf("A large residual correlation is found between item %d and item %d: %f\n", row, col, value))
} # Now we detect the problematic pairs.
```

According to this, it seems that there is no local dependency. Let's see how Yen's Q3 statistics look like.

```{r}
q3 <- residuals(fit, type = "Q3") # Running Yen's Q3 statistics
findCorrelation(q3, cutoff = 0.2, verbose = T) # Flagging problematic item pairwise correlations.
```

This code asks R to run `residuals()` function with `"Q3"` as the `type`, and then keep the value as a new matrix, namely `q3`.

As we see here in the console that items Q3, Q4, Q5, Q7, Q11, Q13, Q14, Q18, Q19, and Q21 are problematic.

### Step 6: Plots (item characteristics curve, item information curve, and test information curve)

From the previous step, we found some problems with model misspecification. Let's take a look at the curves to closely examine the items.

```{r}
# Plotting Item Probability Functions (CPF)
tracePlot(fit, title = "Item Probability Functions of RWA Scale") + labs(color="Response Categories")
```

The `labs(color)` function is to assign different colors to each curve representing a response category.

All CPFs of RWA items seem to be significantly overlapping, and are peaked on the right side of the curves. This implies that RWA items are more sensitive to differentiate participants with high levels of authoritarian personality.

```{r}
# Plotting Item Information Curve (IIC).
itemInfoPlot(fit, facet = TRUE, title = "Item Information Functions of the RWA Scale")
```

The `facet` option is set to `TRUE` so that R displays IIF for each item. Some items (Q1, Q5, Q6, Q8, Q9, Q11) seem to yield the least information compared to other items.

```{r}
# Plotting Test Information Curve.
testInfoPlot(fit, title="Test Information Function of the RWA Scale")
```

Apparently, the RWA scale is informative to measure participants with theta levels between -2SD to +4SD.

### Step 7: Computing reliability

At last, we want to examine the reliability of the RWA scale. In IRT, there are two ways to estimate reliability, which are: marginal and empirical reliability.

To calculate empirical reliability, we have to compute participants' theta (factor scores) estimated by the model. To do this, we only need to run a simple script below:

```{r}
theta_se <- fscores(fit, full.scores.SE = T) # Extracting the estimated theta score of each participant.
```

The `fscores()` function is used to compute the estimated $\theta$ value for each participant predicted by the `fit` model. The `full.scores.SE` is set to `TRUE` because the package uses the $\theta$ value and standard error of each participant (*SE*$\theta$) to compute the empirical reliability. We store the estimated $\theta$ and its *SE*$\theta$ in a new data frame, namely `theta_se`. And then, we can use the estimated theta to compute empirical reliability.

```{r}
empirical_rxx(theta_se) # Then use the estimated theta to calculate empirical reliability.
```

Reliability seems to be pretty good. Now, we need to calculate marginal reliability.

```{r}
marginal_rxx(fit) # Calculating marginal reliability.
# Marginal reliability is almost similar to empirical reliability.
```

...and then plot the marginal reliability across the latent trait spectrum.

```{r}
conRelPlot(fit, title="Reliability of the RWA Scale Given to the θ Level")
```

The RWA scale seems to be optimal (having sufficient reliability, \~0.75) for measuring participants with theta levels between -2SD and +4SD.

Let's compare IRT-based reliability and unweighted sum-score based reliability.

```{r}
omega(data) # This script computes several reliability coefficients
```

As we see here, the RWA scale is internally consistent.

## THE SECOND PART: Simulated Dataset

To give readers a clearer picture of the implementation of GRM, in the second part, we will demonstrate how to simulate a dataset using `simdata()` function in `mirt` package. We will simulate a new dataset using model and item parameters that we estimated with the real data in **the first part** so that readers can compare the first (real) and the second (ideal) model.

### Step 1: Simulate the dataset using `mirt` package

Simulating a data set should always start with specifying the parameters, and to that end, we will replicate a dataset using model and item parameters that we have estimated in **the first part**. `mirt` package also provides the option to simulate response patterns (i.e., scale data) from a custom input matrix of theta, but we don't include the example here in the material. Those who are interested in simulating response patterns from a custom theta matrix are advised to read the examples provided in [`mirt` documentation](https://cran.r-project.org/web/packages/mirt/mirt.pdf).

It's important to remember that the simulated data is constructed by strictly adhering to the model assumptions and parameters. Therefore, model and item parameters here are just statistical artifacts that should not be taken at face value. The purpose of implementing GRM with simulated data here is to show how the "ideal" scenario looks like.

To simulate response patterns from estimated RWA model we have shown in the first part, type the codes below.

```{r}
sim_data <- simdata(model=fit, N=800) # simulating 800 response patterns from RWA scale we previously estimated in the first part
sim_data <- as.data.frame(sim_data) # and then set the data to a data frame
```

We now have a newly created data frame, `sim_data`, containing 800 response patterns estimated from the model and item parameters we already had.

Now, let's take a glimpse at the dataset.

```{r}
describe(sim_data) # descriptive stats of items in the dataset
```

The items look good and are coded correctly. We don't need to reverse any items because our dataset is built upon a GRM model estimated from a cleaned dataset.

Then we can continue to examine the dimensionality of the simulated dataset.

### Step 2: Examining dimensionality

In the first part, we saw that `irt.fa()` and factor analysis with polychoric correlations are not needed when response categories of the scale are more than 8. Therefore, we will examine the dimensionality by running an exploratory factor analysis, just as we did in the previous part.

```{r}
cor_sim <- cor(sim_data, method="spearman") # First, creating a (Spearman) correlation matrix.
efa_sim <- fa(cor_sim, nfactors=1, fm="minres") # Now, running exploratory factor analysis. 
print(efa_sim) # Print the results.
```

Let's take a closer look at the output.

Note that the simulated response patterns are a different set of data than we had in the first part, even though they were constructed using the model and item parameters estimated in the first part. Therefore, the numbers here are changed, but as we can see here, the unidimensionality evidence remains. All items are loaded strongly on to one latent dimension as no loadings are smaller than `0.3`. Proportion of total variance explained by the latent factor is `0.54`, which further shows that the simulated data is unidimensional.

```{r}
plot(efa_sim$values, type = "b", main = "Scree Plot", xlab = "Factor", ylab = "Eigenvalue") # Scree plot
abline(h = 1, col = "red", lty = 2) # Add new line to eigenvalue = 1.
```

As we can see in the plot, the eigenvalue levels off exactly after one factor, which further supports our assumption. However, let's do a parallel analysis as we did in the first part.

```{r}
pa <- fa.parallel(sim_data, nfactors = 1, fm="minres", fa="fa") # Running a parallel analysis.
pa$fa.values # Seeing the eigenvalues of each factor.
```

Again, from the plot and the eigenvalues (`11.9/0.21`), we can conclude that one latent factor is sufficient for the simulated data.

### Step 3: Model estimation, parameters, and fit statistics

```{r}
sim_model <- 'theta=1-22' # specifiying the model
fit_sim <- mirt(data=sim_data, 1, model=model, itemtype="graded", SE=T, verbose=F) # estimating the GRM model
summary(fit_sim) # # Displaying factor loadings and commonality.
```

Compared to the real dataset, the loadings here don't differ much. We can see here that all items are properly loaded on to a single latent factor. However, it's interesting to see if the overall model fit differs from the real dataset we analyzed in the first part.

```{r}
M2(fit_sim, type="C2")
```

As we can see here, the model fit indices appear to be perfect, unlike the previous model we had.

Let's now look at the item fit indices by running this simple command below.

```{r}
itemfit(fit_sim)
```

Similarly, the item fit indices appear to be perfect. All RMSEA values are below `0.08` and the *p* values are also above `0.05`. Of course, this is not surprising since we are analyzing simulated response patterns, not a real data.

Next, we will look at the IRT plots as we did in the previous part. We will skip the residual analysis part because it is not necessary since the model we estimated here is *near* perfect.

### Step 4: Plots (item characteristics curve, item information curve, and test information curve)

To examine the items in detail, we can start by plotting the item probability functions by running the following line of code.

```{r}
# Plotting Item Probability Functions (CPF)
tracePlot(fit_sim, title = "Item Probability Functions of the Simulated Dataset") + labs(color="Response Categories")
```

Similarly, the CPFs overlap significantly and peak on the right side of the curves. This suggests that our simulated dataset reflects the real dataset, in which items are more sensitive to discriminating participants with high theta, but much less sensitive to discriminating participants whose theta is below the mean.

```{r}
# Plotting Item Information Curve (IIC).
itemInfoPlot(fit_sim, facet = TRUE, title = "Item Information Functions of the Simulated Dataset")
```

Similar to the real data, some items (Q1, Q5, Q6, Q8, Q9, Q11, Q20) seem to yield the least information compared to other items.

Now let's see what happens when we combine item information functions into a test information function.

```{r}
# Plotting Test Information Curve.
testInfoPlot(fit_sim, title="Test Information Function of the Simulated Dataset")
```

Similarly, the RWA scale is informative to measure participants with theta levels between -2SD to +4SD.

### Step 5: Computing reliability

At last, we want to examine the reliability of the RWA scale. In IRT, there are two ways to estimate reliability, which are: marginal and empirical reliability.

To calculate empirical reliability, we have to compute participants' theta (factor scores) estimated by the model. To do this, we only need to run a simple script below:

```{r}
theta_se_sim <- fscores(fit_sim, full.scores.SE = T) # Extracting the estimated theta score of each participant.
empirical_rxx(theta_se_sim) # Then use the estimated theta to calculate empirical reliability.
```

Reliability is slightly different from the real data, but still pretty good.

Now, we need to calculate marginal reliability and then plot the marginal reliability across the latent trait spectrum.

```{r}
marginal_rxx(fit_sim) # Calculating marginal reliability.
conRelPlot(fit_sim, title="Reliability of the Simulated Dataset Given to the θ Level")
```

The simulated data shows the same pattern as the real one, which shows that the scale as a whole is optimal (having sufficient reliability, \~0.8) for measuring participants with theta levels between -2SD and +4SD.

Thank you for getting through this tutorial until the very end! We hope that this tutorial is useful!

If you have any feedback, please get in touch and let us know by email here: [amelia.zein\@psikologi.unair.ac.id](mailto:amelia.zein@psikologi.unair.ac.id).

# Session Info

```{r}
sessionInfo()
```
